{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107555,"databundleVersionId":13033998,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FDS Challenge\n\nThis notebook will guide you through the first steps of the competition. Our goal here is to show you how to:\n\n1.  Load the `train.jsonl` and `test.jsonl` files from the competition data.\n2.  Create a very simple set of features from the data.\n3.  Train a basic model.\n4.  Generate a `submission.csv` file in the correct format.\n5.  Submit your results.\n\nLet's get started!","metadata":{"_uuid":"00713b04-49ef-4f25-9036-4177259b53e4","_cell_guid":"9cf7fcc7-f8d3-4f9a-9128-f94e5a717ff3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# 1. Loading and Inspecting the Data","metadata":{"_uuid":"30695d23-3c14-4ded-ac54-0da74eab1161","_cell_guid":"b7c3e856-84d8-4080-97ee-7570902defbc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport os\n\n# --- Define the path to our data ---\nCOMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\nDATA_PATH = os.path.join('../input', COMPETITION_NAME)\n\ntrain_file_path = os.path.join(DATA_PATH, 'train.jsonl')\ntest_file_path = os.path.join(DATA_PATH, 'test.jsonl')\ntrain_data = []\n\n# Read the file line by line\nprint(f\"Loading data from '{train_file_path}'...\")\ntry:\n    with open(train_file_path, 'r') as f:\n        for line in f:\n            # json.loads() parses one line (one JSON object) into a Python dictionary\n            train_data.append(json.loads(line))\n\n    print(f\"Successfully loaded {len(train_data)} battles.\")\n\n    # Let's inspect the first battle to see its structure\n    print(\"\\n--- Structure of the first train battle: ---\")\n    if train_data:\n        first_battle = train_data[0]\n        \n        # To keep the output clean, we can create a copy and truncate the timeline\n        battle_for_display = first_battle.copy()\n        battle_for_display['battle_timeline'] = battle_for_display.get('battle_timeline', [])[:2] # Show first 2 turns\n        \n        # Use json.dumps for pretty-printing the dictionary\n        print(json.dumps(battle_for_display, indent=4))\n        if len(first_battle.get('battle_timeline', [])) > 3:\n            print(\"    ...\")\n            print(\"    (battle_timeline has been truncated for display)\")\n\n\nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find the training file at '{train_file_path}'.\")\n    print(\"Please make sure you have added the competition data to this notebook.\")","metadata":{"_uuid":"a0e3bcf2-ab61-499f-b5e6-c98aa1b68d6e","_cell_guid":"4151dbde-ac63-44b6-bc6a-dec9e4887644","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T22:02:24.882001Z","iopub.execute_input":"2025-11-12T22:02:24.883029Z","iopub.status.idle":"2025-11-12T22:02:36.425647Z","shell.execute_reply.started":"2025-11-12T22:02:24.882991Z","shell.execute_reply":"2025-11-12T22:02:36.424702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Features Engineering","metadata":{"_uuid":"3a8ee9c9-3a1f-441a-9c71-6bd1445de82f","_cell_guid":"0e496703-c5aa-4a03-8262-f8f9c1d2f386","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# =========================\n# CELLA 2 â€” Feature Engineering \n# =========================\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport json\nfrom typing import List, Dict, Any, Tuple\nfrom collections import Counter, defaultdict\n\n# ---------------------------------------------\n# Base stat keys used throughout the feature extraction\n# ---------------------------------------------\nBASE_STAT_KEYS = [\"base_hp\",\"base_atk\",\"base_def\",\"base_spa\",\"base_spd\",\"base_spe\"]\n\n# ---------------------------------------------\n# Static team composition and stats\n# ---------------------------------------------\ndef unique_types(team: List[Dict[str, Any]]) -> int:\n    collected=[]\n    for p in team or []:\n        ts=p.get(\"types\") or []\n        if isinstance(ts,str): ts=[ts]\n        collected.extend([t for t in ts if t])\n    return len(set(collected))\n\ndef sum_stats_of_team(team: List[Dict[str, Any]]) -> float:\n    total=0.0\n    for p in team or []:\n        for k in BASE_STAT_KEYS:\n            v=p.get(k)\n            if isinstance(v,(int,float)):\n                total+=float(v)\n    return total\n\ndef avg_stats_of_team(team: List[Dict[str, Any]]) -> float:\n    if not team:\n        return 0.0\n    per=[]\n    for p in team:\n        vals=[p.get(k) for k in BASE_STAT_KEYS if isinstance(p.get(k),(int,float))]\n        if vals:\n            per.append(sum(vals)/len(vals))\n    return float(sum(per)/len(per)) if per else 0.0\n\ndef sum_and_avg_of_single(poke: dict) -> Tuple[float, float]:\n    vals = [poke.get(k) for k in BASE_STAT_KEYS if isinstance(poke.get(k), (int, float))]\n    if not vals:\n        return 0.0, 0.0\n    total = float(sum(vals))\n    return total, total / len(vals)\n\ndef team_stat_variance(team: List[Dict[str,Any]]) -> float:\n    if not team:\n        return 0.0\n    per=[]\n    for p in team:\n        vals=[p.get(k) for k in BASE_STAT_KEYS if isinstance(p.get(k),(int,float))]\n        if vals:\n            per.append(sum(vals)/len(vals))\n    if len(per)<2:\n        return 0.0\n    return float(pd.Series(per).var())\n\ndef _team_speed_stats(team):\n    \"\"\"Return mean and max base speed over a team.\"\"\"\n    sp = [p.get(\"base_spe\", 0.0) for p in team or [] if isinstance(p.get(\"base_spe\", None), (int, float))]\n    if not sp:\n        return 0.0, 0.0\n    return float(np.mean(sp)), float(np.max(sp))\n\n# ---------------------------------------------\n# General numeric helpers\n# ---------------------------------------------\ndef _safe_mean(arr): \n    return float(np.mean(arr)) if arr else 0.0\n\ndef _safe_ratio(a,b,cap=10.0):\n    r=a/(b+1e-6)\n    if r < 0: r = 0.0\n    if r > cap: r = cap\n    if not np.isfinite(r): r = 0.0\n    return float(r)\n\n# ---------------------------------------------\n# Timeline-based HP feature extraction\n# ---------------------------------------------\ndef get_timeline(r: Dict[str,Any], max_turns: int = 30):\n    tl = r.get(\"battle_timeline\",[]) or []\n    return tl[:max_turns] if isinstance(tl,list) else []\n\ndef _extract_hp_series(tl):\n    p1=[]; p2=[]\n    for t in tl:\n        if not isinstance(t,dict): \n            continue\n        s1=t.get(\"p1_pokemon_state\") or {}\n        s2=t.get(\"p2_pokemon_state\") or {}\n        v1=s1.get(\"hp_pct\"); v2=s2.get(\"hp_pct\")\n        if isinstance(v1,(int,float)) and isinstance(v2,(int,float)):\n            p1.append(float(v1)); p2.append(float(v2))\n    return p1,p2\n\ndef _mean_last_std_min(arr):\n    if not arr:\n        return 0.0,0.0,0.0,0.0\n    x=np.array(arr,dtype=float)\n    return float(x.mean()), float(x[-1]), float(x.std(ddof=0)), float(x.min())\n\ndef _window(arr,n): return arr[:n] if arr else []\ndef _frac_positive(arr): return float((np.array(arr)>0).mean()) if arr else 0.0\ndef _slope(arr):\n    if len(arr)<2: return 0.0\n    x=np.arange(len(arr))\n    m,_=np.polyfit(x,np.array(arr),1)\n    return float(m)\ndef _auc_pct(arr): return float(np.sum(arr)/(100.0*len(arr))) if arr else 0.0\ndef _status_count(tl,who):\n    cnt=0\n    k=f\"{who}_pokemon_state\"\n    for t in tl:\n        if not isinstance(t,dict): continue\n        st=(t.get(k) or {}).get(\"status\",None)\n        if st not in (None,\"\",\"none\",\"NONE\"):\n            cnt+=1\n    return float(cnt)\ndef _ko_count(arr): return float(sum(1 for v in arr if v==0))\n\n# ---------------------------------------------\n# Move-related statistics from timeline\n# ---------------------------------------------\ndef _move_stats_for_side(tl, who, window=None):\n    key=f\"{who}_move_details\"\n    seq = tl if window is None else tl[:window]\n    pw, ac, pr = [], [], []\n    for t in seq:\n        md=t.get(key) or {}\n        bp=md.get(\"base_power\"); acc=md.get(\"accuracy\"); pri=md.get(\"priority\")\n        if isinstance(bp,(int,float)): pw.append(float(bp))\n        if isinstance(acc,(int,float)): ac.append(float(acc))\n        if isinstance(pri,(int,float)): pr.append(float(pri))\n    suf=\"\" if window is None else f\"_{window}\"\n    return {\n        f\"mv_{who}_power_mean{suf}\": _safe_mean(pw),\n        f\"mv_{who}_acc_mean{suf}\":   _safe_mean(ac),\n        f\"mv_{who}_priority_mean{suf}\": _safe_mean(pr),\n    }\n# ---------------------------------------------\n# Type effectiveness helpers (uppercase canonical)\n# ---------------------------------------------\n_TYPE_CHART = {\n    \"NORMAL\":   {\"ROCK\":0.5, \"GHOST\":0.0, \"STEEL\":0.5},\n    \"FIRE\":     {\"FIRE\":0.5, \"WATER\":0.5, \"GRASS\":2.0, \"ICE\":2.0, \"BUG\":2.0, \"ROCK\":0.5, \"DRAGON\":0.5, \"STEEL\":2.0},\n    \"WATER\":    {\"FIRE\":2.0, \"WATER\":0.5, \"GRASS\":0.5, \"GROUND\":2.0, \"ROCK\":2.0, \"DRAGON\":0.5},\n    \"ELECTRIC\": {\"WATER\":2.0, \"ELECTRIC\":0.5, \"GRASS\":0.5, \"GROUND\":0.0, \"FLYING\":2.0, \"DRAGON\":0.5},\n    \"GRASS\":    {\"FIRE\":0.5, \"WATER\":2.0, \"GRASS\":0.5, \"POISON\":0.5, \"GROUND\":2.0, \"FLYING\":0.5, \"BUG\":0.5, \"ROCK\":2.0, \"DRAGON\":0.5, \"STEEL\":0.5},\n    \"ICE\":      {\"FIRE\":0.5, \"WATER\":0.5, \"GRASS\":2.0, \"GROUND\":2.0, \"FLYING\":2.0, \"DRAGON\":2.0, \"STEEL\":0.5},\n    \"FIGHTING\": {\"NORMAL\":2.0, \"ICE\":2.0, \"POISON\":0.5, \"FLYING\":0.5, \"PSYCHIC\":0.5, \"BUG\":0.5, \"ROCK\":2.0, \"GHOST\":0.0, \"DARK\":2.0, \"STEEL\":2.0, \"FAIRY\":0.5},\n    \"POISON\":   {\"GRASS\":2.0, \"POISON\":0.5, \"GROUND\":0.5, \"ROCK\":0.5, \"GHOST\":0.5, \"STEEL\":0.0, \"FAIRY\":2.0},\n    \"GROUND\":   {\"FIRE\":2.0, \"ELECTRIC\":2.0, \"GRASS\":0.5, \"POISON\":2.0, \"FLYING\":0.0, \"BUG\":0.5, \"ROCK\":2.0, \"STEEL\":2.0},\n    \"FLYING\":   {\"ELECTRIC\":0.5, \"GRASS\":2.0, \"FIGHTING\":2.0, \"BUG\":2.0, \"ROCK\":0.5, \"STEEL\":0.5},\n    \"PSYCHIC\":  {\"FIGHTING\":2.0, \"POISON\":2.0, \"PSYCHIC\":0.5, \"DARK\":0.0, \"STEEL\":0.5},\n    \"BUG\":      {\"FIRE\":0.5, \"GRASS\":2.0, \"FIGHTING\":0.5, \"POISON\":0.5, \"FLYING\":0.5, \"PSYCHIC\":2.0, \"GHOST\":0.5, \"DARK\":2.0, \"STEEL\":0.5, \"FAIRY\":0.5},\n    \"ROCK\":     {\"FIRE\":2.0, \"ICE\":2.0, \"FIGHTING\":0.5, \"GROUND\":0.5, \"FLYING\":2.0, \"BUG\":2.0, \"STEEL\":0.5},\n    \"GHOST\":    {\"NORMAL\":0.0, \"PSYCHIC\":2.0, \"GHOST\":2.0, \"DARK\":0.5},\n    \"DRAGON\":   {\"DRAGON\":2.0, \"STEEL\":0.5, \"FAIRY\":0.0},\n    \"DARK\":     {\"FIGHTING\":0.5, \"PSYCHIC\":2.0, \"GHOST\":2.0, \"DARK\":0.5, \"FAIRY\":0.5},\n    \"STEEL\":    {\"FIRE\":0.5, \"WATER\":0.5, \"ELECTRIC\":0.5, \"ICE\":2.0, \"ROCK\":2.0, \"FAIRY\":2.0, \"STEEL\":0.5},\n    \"FAIRY\":    {\"FIRE\":0.5, \"FIGHTING\":2.0, \"POISON\":0.5, \"DRAGON\":2.0, \"DARK\":2.0, \"STEEL\":0.5},\n}\n\ndef _type_multiplier(move_type: str, target_types: List[str] | set) -> float:\n    \"\"\"Effectiveness multiplier for move_type against mono/dual target types.\"\"\"\n    if not move_type:\n        return 1.0\n    mt = move_type.strip().upper()\n    mult = 1.0\n    for tt in target_types or []:\n        tt_up = str(tt).strip().upper()\n        mult *= _TYPE_CHART.get(mt, {}).get(tt_up, 1.0)\n    return float(mult) if np.isfinite(mult) else 1.0\n\ndef _avg_type_eff_p1_vs_p2lead(tl: list[dict], p2_lead_types: List[str] | set, window: int | None = None) -> float:\n    \"\"\"Mean effectiveness of P1 used moves vs P2 lead types over full/early window.\"\"\"\n    seq = tl if window is None else tl[:window]\n    vals = []\n    for t in seq:\n        md = t.get(\"p1_move_details\") or {}\n        mv_t = md.get(\"type\")\n        if isinstance(mv_t, str) and p2_lead_types:\n            vals.append(_type_multiplier(mv_t, p2_lead_types))\n    return float(np.mean(vals)) if vals else 1.0  # neutral if unknown\n\n# ---------------------------------------------\n# STAB features (Same-Type Attack Bonus)\n# ---------------------------------------------\ndef _name_to_types_map_p1(record: Dict[str, Any]) -> Dict[str, set]:\n    mp = {}\n    for p in record.get(\"p1_team_details\", []) or []:\n        nm = (p.get(\"name\") or \"\").strip().lower()\n        ts = p.get(\"types\") or []\n        if isinstance(ts, str):\n            ts = [ts]\n        ts_norm = {str(t).strip().upper() for t in ts if t and str(t).strip().upper() != \"NOTYPE\"}\n        if nm:\n            mp[nm] = ts_norm\n    return mp\n\ndef _active_name_and_move_type(turn: Dict[str, Any], who: str) -> tuple[str, str]:\n    state = turn.get(f\"{who}_pokemon_state\") or {}\n    md    = turn.get(f\"{who}_move_details\") or {}\n    nm = (state.get(\"name\") or \"\").strip().lower()\n    mv_t = (md.get(\"type\") or \"\").strip().upper()\n    return nm, mv_t\n\ndef _stab_features(record: Dict[str, Any], max_turns: int = 30) -> Dict[str, float]:\n    tl = get_timeline(record, max_turns=max_turns)\n\n    # type maps of P1 (name -> set(types))\n    p1_types_map = _name_to_types_map_p1(record)\n\n    # ratio & diff - helpers\n    def _accumulate(seq):\n        p1_total = p1_stab = 0\n        p2_total = p2_stab = 0  \n\n        for t in seq:\n            # P1\n            nm1, mv1_type = _active_name_and_move_type(t, \"p1\")\n            if mv1_type:\n                p1_total += 1\n                types1 = p1_types_map.get(nm1, set())\n                is_stab = (mv1_type in types1) if types1 else False\n                if is_stab:\n                    p1_stab += 1\n\n        p1_ratio = (p1_stab / p1_total) if p1_total > 0 else 0.0\n        p2_ratio = 0.0\n\n        return {\n            \"stab_stab_ratio_diff\": float(p1_ratio - p2_ratio),\n            \"stab_stab_ratio_ratio\": _safe_ratio(p1_ratio, p2_ratio if p2_ratio > 0 else 1e-6, cap=10.0),\n        }\n\n    full = _accumulate(tl)\n    w5   = _accumulate(tl[:5])\n\n    return {\n        \"stab_stab_ratio_diff_full\":  float(full[\"stab_stab_ratio_diff\"]),\n        \"stab_stab_ratio_ratio_full\": float(full[\"stab_stab_ratio_ratio\"]),\n        \"stab_stab_ratio_diff_w5\":    float(w5[\"stab_stab_ratio_diff\"]),\n        \"stab_stab_ratio_ratio_w5\":   float(w5[\"stab_stab_ratio_ratio\"]),\n    }\n\n# ---------------------------------------------\n# Early momentum (first 3 turns)\n# ---------------------------------------------\ndef _first_ko_flag(hp_series: list[float]) -> int:\n    for v in hp_series:\n        if isinstance(v, (int, float)) and float(v) == 0.0:\n            return 1\n    return 0\n\ndef _first_status_advantage(tl: list[dict], first_n: int = 3) -> float:\n    p1 = p2 = 0\n    for t in tl[:first_n]:\n        s1 = (t.get(\"p1_pokemon_state\") or {}).get(\"status\", None)\n        s2 = (t.get(\"p2_pokemon_state\") or {}).get(\"status\", None)\n        if s1 not in (None, \"\", \"none\", \"NONE\"): p1 += 1\n        if s2 not in (None, \"\", \"none\", \"NONE\"): p2 += 1\n    return float(p1 - p2)\n\ndef _early_momentum_features(record: Dict[str, Any], first_n: int = 3) -> Dict[str, float]:\n    tl = get_timeline(record, max_turns=30)\n    p1, p2 = _extract_hp_series(tl)\n    p1w, p2w = _window(p1, first_n), _window(p2, first_n)\n\n    diffw = [a - b for a, b in zip(p1w, p2w)] if p1w and p2w and len(p1w) == len(p2w) else []\n    mean_diff_first = float(np.mean(diffw)) if diffw else 0.0\n\n    p1_first_ko = _first_ko_flag(p2w)\n    p2_first_ko = _first_ko_flag(p1w)\n    first_ko_score = float(p1_first_ko - p2_first_ko)\n\n    status_adv = _first_status_advantage(tl, first_n=first_n)\n\n    return {\n        f\"early_hp_diff_mean_{first_n}\": mean_diff_first,\n        f\"early_first_ko_score_{first_n}\": first_ko_score,\n        f\"early_status_advantage_{first_n}\": status_adv,\n    }\n\n# ---------------------------------------------\n# Priority counts and advantage (full / 5 / 10)\n# ---------------------------------------------\ndef _priority_counts(record: Dict[str, Any], max_turns: int = 30, window: int | None = None) -> Dict[str, float]:\n    tl = get_timeline(record, max_turns=max_turns)\n    turns = tl if window is None else tl[:window]\n\n    p1_count = 0.0\n    p2_count = 0.0\n    for t in turns:\n        md1 = t.get(\"p1_move_details\") or {}\n        md2 = t.get(\"p2_move_details\") or {}\n        pri1 = md1.get(\"priority\")\n        pri2 = md2.get(\"priority\")\n        if isinstance(pri1, (int, float)) and float(pri1) > 0: p1_count += 1.0\n        if isinstance(pri2, (int, float)) and float(pri2) > 0: p2_count += 1.0\n\n    suf = \"\" if window is None else f\"_{window}\"\n    return {\n        f\"mv_p1_priority_count{suf}\": p1_count,\n        f\"mv_p2_priority_count{suf}\": p2_count,\n        f\"mv_priority_count_diff{suf}\": p1_count - p2_count,\n    }\n\ndef _priority_feature_block(record: Dict[str, Any]) -> Dict[str, float]:\n    f = {}\n    f.update(_priority_counts(record, max_turns=30, window=None))\n    f.update(_priority_counts(record, max_turns=30, window=5))\n    return f\n\n\n# ====================================\n# LEAD MATCHUP / DAMAGE-INDEX HELPERS\n# ====================================\ndef _simple_damage_index(base_power: float, stab: bool, eff: float, atk_proxy: float, def_proxy: float) -> float:\n    if not isinstance(base_power, (int, float)) or base_power <= 0:\n        return 0.0\n    s = 1.5 if stab else 1.0\n    ratio = (float(atk_proxy) + 1e-3) / (float(def_proxy) + 1e-3)\n    val = float(base_power) * s * float(eff) * ratio\n    return float(val) if np.isfinite(val) else 0.0\n\ndef _p1_vs_p2lead_matchup_index(record: dict, tl: list[dict]) -> dict:\n    p1_team = record.get(\"p1_team_details\", []) or []\n    p1_mean_atk = float(np.mean([p.get(\"base_atk\", 0) for p in p1_team])) if p1_team else 0.0\n    p1_mean_spa = float(np.mean([p.get(\"base_spa\", 0) for p in p1_team])) if p1_team else 0.0\n\n    lead = record.get(\"p2_lead_details\") or {}\n    p2_types = lead.get(\"types\") or []\n    if isinstance(p2_types, str): p2_types = [p2_types]\n    p2_types = [t for t in p2_types if t]\n    p2_def = float(lead.get(\"base_def\", 0.0) or 0.0)\n    p2_spd = float(lead.get(\"base_spd\", 0.0) or 0.0)\n\n    p1map = {}\n    for p in p1_team:\n        nm = (p.get(\"name\") or \"\").strip().lower()\n        ts = p.get(\"types\") or []\n        if isinstance(ts, str): ts = [ts]\n        p1map[nm] = {str(x).strip().upper() for x in ts if x}\n\n    def _acc(window=None):\n        seq = tl if window is None else tl[:window]\n        vals = []\n        for t in seq:\n            md = t.get(\"p1_move_details\") or {}\n            bp = md.get(\"base_power\"); cat = md.get(\"category\"); mtype = md.get(\"type\")\n            if not isinstance(bp, (int, float)) or bp <= 0: \n                continue\n            nm = (t.get(\"p1_pokemon_state\") or {}).get(\"name\", \"\")\n            nm = (nm or \"\").strip().lower()\n            is_stab = str(mtype or \"\").strip().upper() in p1map.get(nm, set())\n            eff = _type_multiplier(mtype, p2_types)\n            if (cat or \"\").upper() == \"PHYSICAL\":\n                idx = _simple_damage_index(bp, is_stab, eff, p1_mean_atk, p2_def)\n            elif (cat or \"\").upper() == \"SPECIAL\":\n                idx = _simple_damage_index(bp, is_stab, eff, p1_mean_spa, p2_spd)\n            else:\n                idx = 0.0\n            vals.append(idx)\n        return float(np.mean(vals)) if vals else 0.0\n\n    return {\n        \"lead_matchup_p1_index_full\": _acc(None),\n        \"lead_matchup_p1_index_5\":    _acc(5),\n    }\n\n# ==========================\n# SWITCH / HAZARD / MOMENTUM\n# ==========================\ndef _switch_count(tl: list[dict], who: str) -> float:\n    last = None\n    cnt = 0\n    key = f\"{who}_pokemon_state\"\n    for t in tl:\n        nm = (t.get(key) or {}).get(\"name\")\n        if nm is None:\n            continue\n        if last is not None and nm != last:\n            cnt += 1\n        last = nm\n    return float(cnt)\n\nHAZARD_MOVES = {\"stealthrock\", \"spikes\", \"toxicspikes\", \"stickyweb\"}\n\ndef _hazard_flags(tl: list[dict]) -> dict:\n    p1 = p2 = 0.0\n    for t in tl:\n        m1 = (t.get(\"p1_move_details\") or {}).get(\"name\")\n        m2 = (t.get(\"p2_move_details\") or {}).get(\"name\")\n        if m1 and str(m1).strip().lower() in HAZARD_MOVES: p1 = 1.0\n        if m2 and str(m2).strip().lower() in HAZARD_MOVES: p2 = 1.0\n    return {\"hazard_p1_flag\": p1, \"hazard_p2_flag\": p2, \"hazard_flag_diff\": p1 - p2}\n\ndef _momentum_shift(tl: list[dict], t1: int = 3, t2: int = 10) -> dict:\n    def _hp_diff_mean(win):\n        p1, p2 = _extract_hp_series(win)\n        if not p1 or not p2 or len(p1) != len(p2): return 0.0\n        d = [a-b for a,b in zip(p1,p2)]\n        return float(np.mean(d)) if d else 0.0\n    d1 = _hp_diff_mean(tl[:t1]); d2 = _hp_diff_mean(tl[:t2])\n    return {\"momentum_shift_3_10\": float(d1 - d2), \"momentum_shift_abs_3_10\": float(abs(d1 - d2))}\n\nHEAL_MOVES = {\"recover\",\"roost\",\"softboiled\",\"rest\",\"wish\",\"synthesis\",\"morningsun\",\"moonlight\",\"drainpunch\",\"leechseed\"}\n\ndef _recovery_pressure(tl: list[dict]) -> dict:\n    p1 = p2 = 0.0\n    for t in tl:\n        m1 = (t.get(\"p1_move_details\") or {}).get(\"name\")\n        m2 = (t.get(\"p2_move_details\") or {}).get(\"name\")\n        if m1 and str(m1).strip().lower() in HEAL_MOVES: p1 += 1.0\n        if m2 and str(m2).strip().lower() in HEAL_MOVES: p2 += 1.0\n    return {\"recover_p1_count\": p1, \"recover_p2_count\": p2, \"recover_count_diff\": p1 - p2}\n\n# ---------------------------------------------\n# NEW FEATURES \n# ---------------------------------------------\ndef new_features(r):\n    tl = get_timeline(r, max_turns=30)\n    p1, p2 = _extract_hp_series(tl)\n    t1 = r.get(\"p1_team_details\", []) or []\n    lead = r.get(\"p2_lead_details\", {}) or {}\n\n    f = {}\n    if len(p1) >= 3 and len(p2) >= 3:\n        f['early_hp_winner'] = 1.0 if np.mean(p1[:3]) > np.mean(p2[:3]) else 0.0\n        f['early_hp_difference'] = np.mean(p1[:3]) - np.mean(p2[:3])\n\n    if p1 and p2:\n        f['final_hp_winner'] = 1.0 if p1[-1] > p2[-1] else 0.0\n        f['final_hp_difference'] = p1[-1] - p2[-1]\n\n    p1_total_stats = sum(p.get(k, 0) for p in t1 for k in BASE_STAT_KEYS)\n    p2_total_stats = sum(lead.get(k, 0) for k in BASE_STAT_KEYS)\n    f['stronger_team'] = 1.0 if p1_total_stats > p2_total_stats else 0.0\n    f['team_strength_gap'] = p1_total_stats - p2_total_stats\n\n    p1_speeds = [p.get('base_spe', 0) for p in t1]\n    p2_speed = lead.get('base_spe', 0)\n    f['faster_team'] = 1.0 if max(p1_speeds, default=0) > p2_speed else 0.0\n    f['speed_advantage'] = max(p1_speeds, default=0) - p2_speed\n    f['num_faster_pokemon'] = sum(1 for s in p1_speeds if s > p2_speed)\n\n    f['p1_danger_count'] = sum(1 for hp in p1 if 0 < hp < 25)\n    f['p2_danger_count'] = sum(1 for hp in p2 if 0 < hp < 25)\n    f['survived_more_danger'] = 1.0 if f['p1_danger_count'] < f['p2_danger_count'] else 0.0\n    return f\n\n# ---------------------------------------------\n# Mirko & Deb\n# ---------------------------------------------\ndef get_defensive_profile(types):\n    \"\"\"\n    Combined defensive multipliers for a defender with 'types' against every attack type.\n    Fixed: use attacking type first, then multiply by defender types.\n    \"\"\"\n    types = types or []\n    if isinstance(types, str): types = [types]\n    types_up = [str(t).strip().upper() for t in types if t]\n\n    combined = {}\n    for atk_type in _TYPE_CHART.keys():\n        mult = 1.0\n        for tdef in types_up:\n            mult *= _TYPE_CHART.get(atk_type, {}).get(tdef, 1.0)\n        combined[atk_type] = float(mult)\n    return combined\n\ndef new_features_mirko(battle):\n    features = {}\n    # Player 1 Team aggregate\n    p1_team = battle.get('p1_team_details', []) or []\n    if p1_team:\n        ratios = []\n        v_hp=[]; v_spe=[]; v_atk=[]; v_def=[]\n        all_types=[]\n        weaknesses=[]; resistances=[]; immunities=[]\n        for p in p1_team:\n            if not isinstance(p, dict): \n                continue\n            off = (p.get(\"base_atk\",0) + p.get(\"base_spa\",0))\n            deff = (p.get(\"base_def\",0) + p.get(\"base_spd\",0))\n            ratios.append(off / deff if deff > 0 else 0.0)\n\n            v_hp.append(p.get('base_hp',0)); v_spe.append(p.get('base_spe',0))\n            v_atk.append(p.get('base_atk',0)); v_def.append(p.get('base_def',0))\n\n            ts = p.get(\"types\") or []\n            if isinstance(ts,str): ts=[ts]\n            all_types.extend([t for t in ts if str(t).lower()!='notype'])\n\n            prof = get_defensive_profile(ts)\n            w = sum(1 for m in prof.values() if m > 1)\n            r = sum(1 for m in prof.values() if 0 < m < 1)\n            i = sum(1 for m in prof.values() if m == 0)\n            weaknesses.append(w); resistances.append(r); immunities.append(i)\n\n        features[\"avg_type_role_ratio\"] = float(np.mean(ratios)) if ratios else 0.0\n        features['p1_var_hp']  = float(np.std(v_hp)) if v_hp else 0.0\n        features['p1_var_spe'] = float(np.std(v_spe)) if v_spe else 0.0\n        features['p1_var_atk'] = float(np.std(v_atk)) if v_atk else 0.0\n        features['p1_var_def'] = float(np.std(v_def)) if v_def else 0.0\n\n        unique_types = len(set(all_types))\n        features['diversity_ratio'] = unique_types / 6.0\n\n        features[\"avg_weaknesses\"] = float(np.mean(weaknesses))  if weaknesses  else 0.0\n        features[\"avg_resistances\"] = float(np.mean(resistances)) if resistances else 0.0\n        features[\"avg_immunities\"] = float(np.mean(immunities))  if immunities  else 0.0\n\n    # P2 lead raw stats\n    p2_lead = battle.get('p2_lead_details') or {}\n    if isinstance(p2_lead, dict) and p2_lead:\n        features['p2_lead_hp']  = p2_lead.get('base_hp', 0)\n        features['p2_lead_spe'] = p2_lead.get('base_spe', 0)\n        features['p2_lead_atk'] = p2_lead.get('base_atk', 0)\n        features['p2_lead_def'] = p2_lead.get('base_def', 0)\n\n    # Voluntary leave counters (None move_details ~ skipped)\n    tl = battle.get('battle_timeline', []) or []\n    idx_none_p2 = [i+1 for i,e in enumerate(tl) if e.get('p2_move_details') is None]\n    idx_none_p1 = [i+1 for i,e in enumerate(tl) if e.get('p1_move_details') is None]\n    def _bucket_count(idxs,a,b): return len([x for x in idxs if a<=x<=b])\n    features['vol_leave_diff_1'] = _bucket_count(idx_none_p1,1,10)  - _bucket_count(idx_none_p2,1,10)\n    features['vol_leave_diff_2'] = _bucket_count(idx_none_p1,11,20) - _bucket_count(idx_none_p2,11,20)\n    features['vol_leave_diff_3'] = _bucket_count(idx_none_p1,21,10**9) - _bucket_count(idx_none_p2,21,10**9)\n\n    # Forced leave heuristics (name change + action executed)\n    def _forced_counts(side_key, move_key):\n        lst=[]\n        for t in tl:\n            lst.append([ (t.get(side_key) or {}).get(\"name\"), (t.get(move_key) is None) ])\n        c1=c2=c3=0\n        for i in range(len(lst)-1):\n            changed = (lst[i+1][0] != lst[i][0])\n            acted   = (lst[i+1][1] == False)\n            turn_idx = i+1\n            if changed and acted:\n                if 1<=turn_idx<=10: c1+=1\n                elif 11<=turn_idx<=20: c2+=1\n                else: c3+=1\n        return c1,c2,c3\n    p1c1,p1c2,p1c3 = _forced_counts(\"p1_pokemon_state\",\"p1_move_details\")\n    p2c1,p2c2,p2c3 = _forced_counts(\"p2_pokemon_state\",\"p2_move_details\")\n    features['forced_leave_diff_1'] = float(p1c1 - p2c1)\n    features['forced_leave_diff_2'] = float(p1c2 - p2c2)\n    features['forced_leave_diff_3'] = float(p1c3 - p2c3)\n\n    # IDs / target\n    features['battle_id'] = battle.get('battle_id')\n    if 'player_won' in battle: features['player_won'] = int(battle['player_won'])\n    return features\n\n# ======= helpers for team & HP & damage stats =======\ndef _pnames_from_p1_team(record):\n    team = record.get(\"p1_team_details\", []) or []\n    names = []\n    for p in team:\n        if isinstance(p, dict):\n            nm = (p.get(\"name\") or \"\").strip().lower()\n            if nm: names.append(nm)\n    return names\n\ndef _pname_from_p2_lead(record):\n    lead = record.get(\"p2_lead_details\") or {}\n    if isinstance(lead, dict):\n        nm = (lead.get(\"name\") or \"\").strip().lower()\n        return nm if nm else None\n    return None\n\ndef build_pokemon_win_stats(train_data, alpha=1.0):\n    games = defaultdict(int); wins = defaultdict(int)\n    for r in train_data:\n        p1_names = _pnames_from_p1_team(r)\n        p2_lead  = _pname_from_p2_lead(r)\n        p1_won   = bool(r.get(\"player_won\", False))\n        for nm in p1_names: games[nm]+=1\n        if p2_lead: games[p2_lead]+=1\n        if p1_won:\n            for nm in p1_names: wins[nm]+=1\n        else:\n            if p2_lead: wins[p2_lead]+=1\n    winrate={}\n    for nm in games:\n        g=games[nm]; w=wins[nm]\n        wr=(w+alpha)/(g+2*alpha)\n        winrate[nm]={\"games\":g,\"wins\":w,\"winrate\":wr}\n    return winrate\n\ndef team_score_from_stats(team_names, stats, default_wr=0.5):\n    vals=[stats.get((nm or \"\").strip().lower(),{}).get(\"winrate\",default_wr) for nm in team_names if nm]\n    return float(np.mean(vals)) if vals else default_wr\n\ndef predict_from_stats(test_record, stats, threshold=0.5):\n    p1_names = _pnames_from_p1_team(test_record)\n    score = team_score_from_stats(p1_names, stats, default_wr=0.5)\n    return (score > threshold), score\n\ndef build_pokemon_hp_stats(train_data):\n    hp_sum=defaultdict(float); hp_count=defaultdict(int)\n    for r in train_data:\n        timeline = r.get(\"battle_timeline\", []) or []\n        if not timeline: continue\n        last_turn = timeline[-1]\n        for player_key in [\"p1_pokemon_state\", \"p2_pokemon_state\"]:\n            name = (last_turn.get(player_key, {}).get(\"name\") or \"\").strip().lower()\n            hp   = last_turn.get(player_key, {}).get(\"hp_pct\", None)\n            if name and isinstance(hp,(int,float)):\n                hp_sum[name]+=float(hp); hp_count[name]+=1\n    stats = {name: {\"count\": hp_count[name], \"hp_mean\": hp_sum[name]/hp_count[name]} for name in hp_sum}\n    return stats\n\ndef team_hp_score(team_names, hp_stats, default_hp=50.0):\n    vals=[]\n    for name in team_names:\n        n=(name or \"\").strip().lower()\n        vals.append(hp_stats.get(n,{}).get(\"hp_mean\", default_hp))\n    return float(np.mean(vals)) if vals else default_hp\n\ndef build_pokemon_avg_damage(train_data):\n    total_damage=defaultdict(float); battles_count=defaultdict(int)\n    for battle in train_data:\n        timeline = battle.get(\"battle_timeline\", []) or []\n        p1_names = [(p.get(\"name\") or \"\").lower() for p in (battle.get(\"p1_team_details\", []) or []) if isinstance(p,dict)]\n        p2_lead  = battle.get(\"p2_lead_details\", {})\n        p2_name  = (p2_lead.get(\"name\") or \"\").lower() if isinstance(p2_lead,dict) else None\n\n        for name in p1_names: battles_count[name]+=1\n        if p2_name: battles_count[p2_name]+=1\n\n        for i in range(1,len(timeline)):\n            prev, curr = timeline[i-1], timeline[i]\n            hp2b = (prev.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\", None)\n            hp2a = (curr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\", None)\n            if isinstance(hp2b,(int,float)) and isinstance(hp2a,(int,float)):\n                dmg=max(0,hp2b-hp2a)\n                if p1_names and dmg>0:\n                    for name in p1_names: total_damage[name]+=dmg\n            hp1b = (prev.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\", None)\n            hp1a = (curr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\", None)\n            if isinstance(hp1b,(int,float)) and isinstance(hp1a,(int,float)):\n                dmg=max(0,hp1b-hp1a)\n                if p2_name and dmg>0:\n                    total_damage[p2_name]+=dmg\n    avg_damage = {name: total_damage[name]/battles_count[name] for name in battles_count if battles_count[name]>0}\n    return avg_damage\n\ndef damage_feature_for_battle(record, avg_damage):\n    p1_names = [(p.get(\"name\") or \"\").lower() for p in (record.get(\"p1_team_details\",[]) or []) if isinstance(p,dict)]\n    p1_damage_score = sum(avg_damage.get(name, 0.0) for name in p1_names)\n    p2_lead = record.get(\"p2_lead_details\", {}) or {}\n    p2_name = (p2_lead.get(\"name\") or \"\").lower() if isinstance(p2_lead,dict) else None\n    p2_damage_score = avg_damage.get(p2_name,0.0) if p2_name else 0.0\n    diff = p1_damage_score - p2_damage_score\n    return {\"avg_damage_p1\": p1_damage_score, \"avg_damage_p2\": p2_damage_score, \"avg_damage_diff\": diff, \"damage_prediction\": 1.0 if diff>0 else 0.0}\n\n# ======= Deb's feature block (kept) =======\ndef new_features_deb(r):\n    tl = get_timeline(r, max_turns=30)\n    p1, p2 = _extract_hp_series(tl)\n    t1 = r.get(\"p1_team_details\", []) or []\n    lead = r.get(\"p2_lead_details\", {}) or {}\n    f = {}\n\n    if len(p1) >= 3 and len(p2) >= 3:\n        media_p1 = float(np.mean(p1[:3])); media_p2 = float(np.mean(p2[:3]))\n        f['is_p1_higher_avg_hp_after_3_turns'] = 1.0 if media_p1 > media_p2 else 0.0\n        f['avg_hp_difference_after_3_turns'] = media_p1 - media_p2\n\n    if p1 and p2:\n        f['is_player1_final_hp_winner'] = 1.0 if p1[-1] > p2[-1] else 0.0\n        f['final_hp_difference'] = p1[-1] - p2[-1]\n\n    if len(p1) >= 6 and len(p2) >= 6:\n        f['comeback_happened'] = float((np.mean(p1[:3]) > np.mean(p2[:3])) != (np.mean(p1[-3:]) > np.mean(p2[-3:])))\n\n    p1_total_stats = sum(p.get('base_hp',0)+p.get('base_atk',0)+p.get('base_def',0)+p.get('base_spa',0)+p.get('base_spd',0)+p.get('base_spe',0) for p in t1 if isinstance(p,dict))\n    p2_total_stats = (lead.get('base_hp',0)+lead.get('base_atk',0)+lead.get('base_def',0)+lead.get('base_spa',0)+lead.get('base_spd',0)+lead.get('base_spe',0))\n    f['stronger_team'] = 1.0 if p1_total_stats > p2_total_stats else 0.0\n    f['team_strength_gap'] = p1_total_stats - p2_total_stats\n\n    p1_speeds = [p.get('base_spe', 0) for p in t1 if isinstance(p,dict)]\n    p2_speed = lead.get('base_spe', 0) if isinstance(lead,dict) else 0\n    if p1_speeds:\n        f['faster_team'] = 1.0 if max(p1_speeds) > p2_speed else 0.0\n        f['speed_advantage'] = max(p1_speeds) - p2_speed\n        f['num_faster_pokemon'] = sum(1 for s in p1_speeds if s > p2_speed)\n    else:\n        f['faster_team'] = 0.0; f['speed_advantage'] = 0.0; f['num_faster_pokemon'] = 0.0\n\n    p1_powers=[]; p2_powers=[]\n    for t in tl:\n        if not isinstance(t,dict): continue\n        md1=t.get('p1_move_details'); md2=t.get('p2_move_details')\n        bp1 = md1.get('base_power') if isinstance(md1,dict) else None\n        bp2 = md2.get('base_power') if isinstance(md2,dict) else None\n        if isinstance(bp1,(int,float)) and bp1>0: p1_powers.append(float(bp1))\n        if isinstance(bp2,(int,float)) and bp2>0: p2_powers.append(float(bp2))\n    if p1_powers and p2_powers:\n        f['most_avg_powerful_move'] = 1.0 if np.mean(p1_powers) > np.mean(p2_powers) else 0.0\n        f['avg_move_power_difference'] = float(np.mean(p1_powers) - np.mean(p2_powers))\n    else:\n        f['most_avg_powerful_move'] = 0.0; f['avg_move_power_difference'] = 0.0\n\n    f['p1_low_hp_count'] = sum(1 for hp in p1 if 0 < hp < 25)\n    f['p2_low_hp_count'] = sum(1 for hp in p2 if 0 < hp < 25)\n    f['is_player1_less_time_in_danger'] = 1.0 if f['p1_low_hp_count'] < f['p2_low_hp_count'] else 0.0\n    f['battle_length'] = len(tl)\n    f['long_battle'] = 1.0 if len(tl) > 15 else 0.0\n\n    if len(p1) > 1 and len(p2) > 1:\n        p1_changes=[abs(p1[i]-p1[i-1]) for i in range(1,len(p1))]\n        p2_changes=[abs(p2[i]-p2[i-1]) for i in range(1,len(p2))]\n        f['p1_hp_stability'] = -float(np.mean(p1_changes)) if p1_changes else 0.0\n        f['p2_hp_stability'] = -float(np.mean(p2_changes)) if p2_changes else 0.0\n        f['more_stable_hp'] = 1.0 if (p1_changes and p2_changes and np.mean(p1_changes) < np.mean(p2_changes)) else 0.0\n    else:\n        f['p1_hp_stability']=0.0; f['p2_hp_stability']=0.0; f['more_stable_hp']=0.0\n\n    p1_first_ko=0.0; p2_first_ko=0.0\n    for hp1,hp2 in zip(p1,p2):\n        if hp2==0 and p2_first_ko==0.0: p2_first_ko=1.0; break\n        if hp1==0 and p1_first_ko==0.0: p1_first_ko=1.0; break\n    f['player1_got_first_ko']=p2_first_ko\n    f['player1_suffered_first_ko']=p1_first_ko\n\n    p1_types=set()\n    for p in t1:\n        if not isinstance(p,dict): continue\n        types=p.get('types',[])\n        if isinstance(types,str): types=[types]\n        p1_types.update(t for t in types if t)\n    f['number_different_types']=len(p1_types)\n    f['team_has_type_variety']=1.0 if len(p1_types)>=4 else 0.0\n\n    if p1 and p2:\n        p1_healthy_ratio = sum(1 for hp in p1 if hp>50)/len(p1)\n        p2_healthy_ratio = sum(1 for hp in p2 if hp>50)/len(p2)\n        f['p1_hp_over_50_ratio']=p1_healthy_ratio\n        f['p2_hp_over_50_ratio']=p2_healthy_ratio\n        f['is_player1_healthier']=1.0 if p1_healthy_ratio>p2_healthy_ratio else 0.0\n    else:\n        f['p1_hp_over_50_ratio']=0.0; f['p2_hp_over_50_ratio']=0.0; f['is_player1_healthier']=0.0\n\n    if len(p1)==len(p2) and p1:\n        turns_ahead=sum(1 for a,b in zip(p1,p2) if a>b)\n        f['turns_in_lead']=float(turns_ahead)\n        f['lead_ratio']=turns_ahead/len(p1)\n        f['dominated_battle']=1.0 if turns_ahead>len(p1)*0.7 else 0.0\n    else:\n        f['turns_in_lead']=0.0; f['lead_ratio']=0.0; f['dominated_battle']=0.0\n\n    p_leave = r.get('battle_timeline', []) or []\n    if p_leave:\n        lst=[]; c1=c2=0\n        for turn in p_leave:\n            lst.append([\n                (turn.get(\"p1_pokemon_state\") or {}).get(\"name\"),\n                (turn.get('p1_move_details') is None),\n                (turn.get(\"p2_pokemon_state\") or {}).get(\"name\"),\n                (turn.get('p2_move_details') is None)\n            ])\n        for i in range(len(lst)-1):\n            if lst[i+1][0]!=lst[i][0] and lst[i+1][1]==False: c1+=1\n            elif lst[i+1][2]!=lst[i][2] and lst[i+1][3]==False: c2+=1\n        f['forced_pokemon_switch_diff']=float(c1-c2)\n    else:\n        f['forced_pokemon_switch_diff']=0.0\n\n    if p_leave:\n        p1_names=set([ (t.get(\"p1_pokemon_state\") or {}).get(\"name\") for t in p_leave if (t.get(\"p1_pokemon_state\") or {}).get(\"name\") ])\n        p2_names=set([ (t.get(\"p2_pokemon_state\") or {}).get(\"name\") for t in p_leave if (t.get(\"p2_pokemon_state\") or {}).get(\"name\") ])\n        f['used_pokemon_diff']=float(len(p1_names)-len(p2_names))\n    else:\n        f['used_pokemon_diff']=0.0\n\n    if p_leave:\n        recent=p_leave[-5:] if len(p_leave)>=5 else p_leave\n        p1r=[(t.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0) for t in recent]\n        p2r=[(t.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0) for t in recent]\n        f['avg_hp_recent_diff']=float(np.mean(p1r)-np.mean(p2r)) if p1r and p2r else 0.0\n    else:\n        f['avg_hp_recent_diff']=0.0\n\n    if p_leave:\n        p1_status=sum(1 for t in p_leave if (t.get(\"p1_pokemon_state\") or {}).get(\"status\",\"nostatus\")!=\"nostatus\")\n        p2_status=sum(1 for t in p_leave if (t.get(\"p2_pokemon_state\") or {}).get(\"status\",\"nostatus\")!=\"nostatus\")\n        f['num_bad_status_diff']=float(p2_status-p1_status)\n    else:\n        f['num_bad_status_diff']=0.0\n\n    if p_leave:\n        last=p_leave[-1]\n        p1f=(last.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0)\n        p2f=(last.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0)\n        f['final_hp_diff']=float(p1f-p2f)\n        p1_alive = 1 if p1f>0 else 0\n        p2_alive = 1 if p2f>0 else 0\n        p1_used=len(set([(t.get(\"p1_pokemon_state\") or {}).get(\"name\") for t in p_leave if (t.get(\"p1_pokemon_state\") or {}).get(\"name\")]))\n        p2_used=len(set([(t.get(\"p2_pokemon_state\") or {}).get(\"name\") for t in p_leave if (t.get(\"p2_pokemon_state\") or {}).get(\"name\")]))\n        team_size=6\n        p1_remaining = team_size - p1_used + p1_alive\n        p2_remaining = team_size - p2_used + p2_alive\n        f['pokemon_remaining_diff']=float(p1_remaining - p2_remaining)\n    else:\n        f['final_hp_diff']=0.0; f['pokemon_remaining_diff']=0.0\n\n    if p_leave and len(p_leave)>=2:\n        total_dmg_dealt=0.0; total_dmg_taken=0.0\n        for i in range(1,len(p_leave)):\n            prev, curr = p_leave[i-1], p_leave[i]\n            p2b=(prev.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0)\n            p2a=(curr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0)\n            total_dmg_dealt += max(0,p2b-p2a)\n            p1b=(prev.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0)\n            p1a=(curr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0)\n            total_dmg_taken += max(0,p1b-p1a)\n        f['damage_ratio'] = float(total_dmg_dealt/total_dmg_taken) if total_dmg_taken>0 else (total_dmg_dealt*10 if total_dmg_dealt>0 else 1.0)\n        f['tot_damage_diff']=float(total_dmg_dealt-total_dmg_taken)\n    else:\n        f['damage_ratio']=1.0; f['tot_damage_diff']=0.0\n\n    if p_leave and len(p_leave)>=6:\n        mid=len(p_leave)//2\n        first=p_leave[:mid]; second=p_leave[mid:]\n        p1e=[(t.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0) for t in first]\n        p2e=[(t.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0) for t in first]\n        p1l=[(t.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0) for t in second]\n        p2l=[(t.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0) for t in second]\n        early_adv = float(np.mean(p1e)-np.mean(p2e)) if p1e and p2e else 0.0\n        late_adv  = float(np.mean(p1l)-np.mean(p2l)) if p1l and p2l else 0.0\n        f['late_game_improvement']=late_adv - early_adv\n        f['late_game_hp_adv']=late_adv\n        f['early_game_hp_adv']=early_adv\n    else:\n        f['late_game_improvement']=0.0; f['late_game_hp_adv']=0.0; f['early_game_hp_adv']=0.0\n\n    if len(p1)>=10 and len(p2)>=10:\n        f['avg_hp_diff_gap'] = float( (np.mean(p1[5:10]) - np.mean(p2[5:10])) - (np.mean(p1[:5]) - np.mean(p2[:5])) )\n\n    if len(p1)>3 and len(p2)>3:\n        f['p1_hp_std']=float(np.std(np.diff(p1)))\n        f['p2_hp_std']=float(np.std(np.diff(p2)))\n\n    if p1 and p2:\n        f['max_hp_deficit_player1'] = float(max(0, max(p2) - min(p1)))\n\n    total_dealt=total_taken=0.0\n    for i in range(1,len(tl)):\n        prev, curr = tl[i-1], tl[i]\n        if not (isinstance(prev,dict) and isinstance(curr,dict)): continue\n        weight = 1.0 + (i/len(tl))*0.5\n        p2_prev=(prev.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",100)\n        p2_curr=(curr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",100)\n        total_dealt += max(0,p2_prev-p2_curr)*weight\n        p1_prev=(prev.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",100)\n        p1_curr=(curr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",100)\n        total_taken += max(0,p1_prev-p1_curr)*weight\n    f['damage_trade_ratio_weighted'] = float(total_dealt/max(1,total_taken))\n\n    if tl:\n        last=tl[-1]\n        p1f=(last.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0)\n        p2f=(last.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0)\n        f['final_hp_advantage']=float(p1f-p2f)\n        f['final_hp_ratio']=float(p1f/max(1,p2f))\n        p1_last_pow=(last.get(\"p1_move_details\") or {}).get(\"base_power\",0)\n        p2_last_pow=(last.get(\"p2_move_details\") or {}).get(\"base_power\",0)\n        f['final_power_advantage']=float(p1_last_pow - p2_last_pow)\n        p1_status=(last.get(\"p1_pokemon_state\") or {}).get(\"status\",\"\")\n        p2_status=(last.get(\"p2_pokemon_state\") or {}).get(\"status\",\"\")\n        f['final_status_advantage']=0.0\n        if p2_status and str(p2_status).lower() not in [\"\",\"none\",\"nostatus\"]: f['final_status_advantage'] += 1.0\n        if p1_status and str(p1_status).lower() not in [\"\",\"none\",\"nostatus\"]: f['final_status_advantage'] -= 1.0\n\n        final_score = 0.0\n        final_score += (p1f - p2f) * 0.5\n        if p1f>0 and p2f==0: final_score += 30.0\n        elif p2f>0 and p1f==0: final_score -= 30.0\n        final_score += (p1_last_pow - p2_last_pow) * 0.15\n        final_score += f['final_status_advantage'] * 5.0\n        f['final_battle_score']=final_score\n        f['final_winning_prob']=1.0/(1.0+np.exp(-final_score/10.0))\n\n        recent=tl[-5:] if len(tl)>=5 else tl\n        diffs=[]\n        for t in recent:\n            p1h=(t.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",0)\n            p2h=(t.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",0)\n            diffs.append(p1h-p2h)\n        if diffs:\n            f['recent_avg_hp_advantage']=float(np.mean(diffs))\n            f['recent_hp_improving']=1.0 if len(diffs)>1 and diffs[-1]>diffs[0] else 0.0\n\n    if 'final_hp_advantage' in f and len(tl)>=5:\n        hp_gap=f['final_hp_advantage']\n        turns=len(tl)\n        comeback=0.35\n        if abs(hp_gap)>50: comeback=0.95\n        elif abs(hp_gap)>30: comeback=0.75\n        elif abs(hp_gap)>15: comeback=0.55\n        if turns<10: comeback*=0.8\n        elif turns>20: comeback*=1.2\n        comeback=min(1.0, comeback)\n        win_prob = 0.5 + (comeback*0.5) if hp_gap>0 else 0.5 - (comeback*0.5)\n        f['comeback_difficulty']=float(comeback)\n        f['predicted_win_prob']=float(win_prob)\n\n    if p1 and p2 and len(tl)>=3:\n        p1_current_hp=p1[-1]; p2_current_hp=p2[-1]\n        pattern_score=0.0\n        p1_kos=sum(1 for hp in p2 if hp==0); p2_kos=sum(1 for hp in p1 if hp==0)\n        ko_adv = p1_kos - p2_kos\n        if ko_adv>=2: pattern_score+=0.3\n        elif ko_adv==1: pattern_score+=0.15\n        elif ko_adv==-1: pattern_score-=0.15\n        elif ko_adv<=-2: pattern_score-=0.3\n        if len(p1)>=5 and len(p2)>=5:\n            p1_trend = np.mean(p1[-3:]) - np.mean(p1[-5:-2])\n            p2_trend = np.mean(p2[-3:]) - np.mean(p2[-5:-2])\n            if p1_trend>5 and p2_trend<-5: pattern_score+=0.2\n            elif p1_trend<-5 and p2_trend>5: pattern_score-=0.2\n\n        p1_used=set(); p2_used=set()\n        for t in tl:\n            if isinstance(t,dict):\n                p1n=(t.get(\"p1_pokemon_state\") or {}).get(\"name\",\"\")\n                p2n=(t.get(\"p2_pokemon_state\") or {}).get(\"name\",\"\")\n                if p1n: p1_used.add(p1n)\n                if p2n: p2_used.add(p2n)\n        team_size=6\n        p1_rem=team_size - len(p1_used) + (1 if p1_current_hp>0 else 0)\n        p2_rem=team_size - len(p2_used) + (1 if p2_current_hp>0 else 0)\n        pokemon_adv = p1_rem - p2_rem\n        if pokemon_adv>=2: pattern_score+=0.35\n        elif pokemon_adv==1: pattern_score+=0.20\n        elif pokemon_adv==-1: pattern_score-=0.20\n        elif pokemon_adv<=-2: pattern_score-=0.35\n\n        f['ko_advantage']=float(ko_adv)\n        f['estimated_pokemon_remaining_p1']=float(p1_rem)\n        f['estimated_pokemon_remaining_p2']=float(p2_rem)\n        f['pokemon_advantage']=float(pokemon_adv)\n        base_prob=0.5\n        if 'final_hp_advantage' in f: base_prob += (f['final_hp_advantage']/100.0)*0.3\n        base_prob += pattern_score*0.4\n        if 'predicted_win_prob' in f: base_prob = base_prob*0.7 + f['predicted_win_prob']*0.3\n        f['final_win_probability']=max(0.0,min(1.0,base_prob))\n\n    if 'final_win_probability' in f:\n        prob=f['final_win_probability']\n        confidence = abs(prob-0.5)*2\n        if f.get('p1_alive_final',0)==1 and f.get('p2_alive_final',0)==0: confidence=0.95\n        elif f.get('p1_alive_final',0)==0 and f.get('p2_alive_final',0)==1: confidence=0.95\n        f['prediction_confidence']=float(confidence)\n        if prob>0.75 and confidence>0.6: f['outcome_prediction']=2.0\n        elif prob>0.6: f['outcome_prediction']=1.0\n        elif prob<0.25 and confidence>0.6: f['outcome_prediction']=-2.0\n        elif prob<0.4: f['outcome_prediction']=-1.0\n        else: f['outcome_prediction']=0.0\n    return f\n\n# ---------------------------------------------\n# Global stats built on train_data \n# ---------------------------------------------\nPOKEMON_STATS    = build_pokemon_win_stats(train_data, alpha=1.0)\nPOKEMON_HP_STATS = build_pokemon_hp_stats(train_data)\npokemon_avg_damage = build_pokemon_avg_damage(train_data)\n\n# ---------------------------------------------\n# Full feature set (static + timeline + moves + Mirko & Deb)\n# ---------------------------------------------\ndef _one_record_features(r):\n    # Static team features\n    t1 = r.get(\"p1_team_details\", []) or []\n    lead = r.get(\"p2_lead_details\", {}) or {}\n    t2 = [lead] if isinstance(lead, dict) and lead else []\n\n    p1sz = len(t1); p2sz = len(t2)\n    p1u  = unique_types(t1); p2u = unique_types(t2)\n    p1s  = sum_stats_of_team(t1); p2s = sum_stats_of_team(t2)\n    p1a  = avg_stats_of_team(t1); p2a = avg_stats_of_team(t2)\n    p2_ls, p2_la = sum_and_avg_of_single(lead) if lead else (0.0, 0.0)\n    p1v  = team_stat_variance(t1)\n\n    f = {\n        \"p1_team_size\": p1sz, \"p2_team_size\": p2sz,\n        \"p1_unique_types\": p1u, \"p2_unique_types\": p2u,\n        \"p1_team_stat_sum\": p1s, \"p2_team_stat_sum\": p2s,\n        \"p1_team_stat_avg\": p1a, \"p2_team_stat_avg\": p2a,\n        \"diff_team_size\": p1sz - p2sz,\n        \"diff_unique_types\": p1u - p2u,\n        \"diff_team_stat_sum\": p1s - p2s,\n        \"diff_team_stat_avg\": p1a - p2a,\n        \"p2_lead_stat_sum\": p2_ls, \"p2_lead_stat_avg\": p2_la,\n        \"p1_sum_minus_p2_lead_sum\": p1s - p2_ls,\n        \"p1_avg_minus_p2_lead_avg\": p1a - p2_la,\n        \"p1_team_stat_var\": p1v,\n        \"ratio_p1_avg_over_p2_lead_avg\": _safe_ratio(p1a, p2_la),\n    }\n\n    # Speed advantage vs p2 lead\n    p1_mean_spe, p1_max_spe = _team_speed_stats(t1)\n    p2_lead_spe = float(lead.get(\"base_spe\", 0.0)) if lead else 0.0\n    faster_cnt = sum(1 for p in t1 if isinstance(p.get(\"base_spe\"),(int,float)) and float(p[\"base_spe\"])>p2_lead_spe)\n    frac_faster = float(faster_cnt)/max(1,len(t1))\n    f.update({\n        \"p1_mean_spe\": p1_mean_spe, \"p1_max_spe\": p1_max_spe, \"p2_lead_spe\": p2_lead_spe,\n        \"spe_mean_adv\": p1_mean_spe - p2_lead_spe, \"spe_max_adv\": p1_max_spe - p2_lead_spe,\n        \"p1_frac_faster_than_p2lead\": frac_faster,\n    })\n\n    # Timeline HP features\n    tl = get_timeline(r, max_turns=30)\n    p1, p2 = _extract_hp_series(tl)\n    diff = [a-b for a,b in zip(p1,p2)] if p1 and p2 and len(p1)==len(p2) else []\n\n    p1m,p1l,p1s_,p1mn = _mean_last_std_min(p1)\n    p2m,p2l,p2s_,p2mn = _mean_last_std_min(p2)\n    dm,dl,ds,dmn = _mean_last_std_min(diff)\n\n    f.update({\n        \"tl_turns_used\": float(len(tl)),\n        \"tl_p1_hp_mean\": p1m, \"tl_p1_hp_last\": p1l, \"tl_p1_hp_std\": p1s_, \"tl_p1_hp_min\": p1mn,\n        \"tl_p2_hp_mean\": p2m, \"tl_p2_hp_last\": p2l, \"tl_p2_hp_std\": p2s_, \"tl_p2_hp_min\": p2mn,\n        \"tl_hp_diff_mean\": dm, \"tl_hp_diff_last\": dl, \"tl_hp_diff_std\": ds, \"tl_hp_diff_min\": dmn,\n        \"tl_p1_hp_slope\": _slope(p1), \"tl_p2_hp_slope\": _slope(p2), \"tl_hp_diff_slope\": _slope(diff),\n        \"tl_p1_hp_auc\": _auc_pct(p1), \"tl_p2_hp_auc\": _auc_pct(p2),\n        \"tl_frac_turns_advantage\": _frac_positive(diff),\n        \"tl_p1_status_count\": _status_count(tl,\"p1\"),\n        \"tl_p2_status_count\": _status_count(tl,\"p2\"),\n    })\n    f[\"tl_status_count\"] = f[\"tl_p1_status_count\"] + f[\"tl_p2_status_count\"]\n    f[\"tl_p1_ko_count\"]  = _ko_count(p1)\n    f[\"tl_p2_ko_count\"]  = _ko_count(p2)\n    f[\"tl_ko_count\"]     = f[\"tl_p1_ko_count\"] + f[\"tl_p2_ko_count\"]\n\n    # Type effectiveness P1 â†’ P2 lead\n    p2_types = lead.get(\"types\") or []\n    if isinstance(p2_types,str): p2_types=[p2_types]\n    p2_types=[t for t in p2_types if t]\n    f.update({\n        \"ter_p1_vs_p2lead_full\": _avg_type_eff_p1_vs_p2lead(tl, p2_types, window=None),\n        \"ter_p1_vs_p2lead_5\":    _avg_type_eff_p1_vs_p2lead(tl, p2_types, window=5),\n        \"ter_p1_vs_p2lead_10\":   _avg_type_eff_p1_vs_p2lead(tl, p2_types, window=10),\n    })\n\n    # --- New safe, bounded features (add near the end of _one_record_features) ---\n\n    # 1) Team offensive tilt: physical vs special (bounded, finite)\n    p1_sum_atk = float(sum(p.get(\"base_atk\", 0) for p in t1 if isinstance(p, dict)))\n    p1_sum_spa = float(sum(p.get(\"base_spa\", 0) for p in t1 if isinstance(p, dict)))\n    f[\"p1_offense_bias_ratio\"] = (p1_sum_atk + 1e-3) / (p1_sum_spa + 1e-3)  # >1 => more physical tilt\n    f[\"p1_offense_balance_gap\"] = p1_sum_atk - p1_sum_spa\n    \n    # 2) Defensive overlap: shared-weakness burden (small integers / means)\n    def _count_weaknesses_of_types(types):\n        prof = get_defensive_profile(types or [])\n        return float(sum(1 for m in prof.values() if m > 1.0))\n    \n    p1_weak_counts = []\n    for p in t1:\n        if isinstance(p, dict):\n            p1_weak_counts.append(_count_weaknesses_of_types(p.get(\"types\", [])))\n    \n    f[\"p1_weakness_mean\"] = float(np.mean(p1_weak_counts)) if p1_weak_counts else 0.0\n    f[\"p1_weakness_max\"]  = float(np.max(p1_weak_counts))  if p1_weak_counts else 0.0\n    \n    # 3) Breadth of resistances (unique resistances union)\n    def _unique_resistances(types):\n        prof = get_defensive_profile(types or [])\n        return {atk for atk, mult in prof.items() if 0.0 < mult < 1.0}\n    \n    res_sets = []\n    for p in t1:\n        if isinstance(p, dict):\n            res_sets.append(_unique_resistances(p.get(\"types\", [])))\n    union_res = set().union(*res_sets) if res_sets else set()\n    f[\"p1_unique_resistances\"] = float(len(union_res))\n    \n    # 4) Early HP volatility (first 5 turns), bounded by [0,100] deltas\n    def _safe_clip_hp(seq):\n        return [max(0.0, min(100.0, float(x))) for x in seq]\n    \n    def _mean_abs_step(arr):\n        return float(np.mean([abs(arr[i] - arr[i-1]) for i in range(1, len(arr))])) if len(arr) > 1 else 0.0\n    \n    p1_hp5 = _safe_clip_hp(_window(p1, 5))\n    p2_hp5 = _safe_clip_hp(_window(p2, 5))\n    f[\"p1_hp_abs_step_5\"] = _mean_abs_step(p1_hp5)\n    f[\"p2_hp_abs_step_5\"] = _mean_abs_step(p2_hp5)\n    f[\"hp_abs_step_gap_5\"] = f[\"p1_hp_abs_step_5\"] - f[\"p2_hp_abs_step_5\"]\n    \n    # 5) Hazards effectiveness given switches (difference version; robust via f.get)\n    haz_p1 = float(f.get(\"hazard_p1_flag\", 0.0))\n    haz_p2 = float(f.get(\"hazard_p2_flag\", 0.0))\n    sw_p1  = float(f.get(\"switch_p1_count\", 0.0))\n    sw_p2  = float(f.get(\"switch_p2_count\", 0.0))\n    haz_sw_p1 = haz_p1 * sw_p2\n    haz_sw_p2 = haz_p2 * sw_p1\n    f[\"hazard_switch_pressure_diff\"] = haz_sw_p1 - haz_sw_p2\n    \n    # 6) Late-game move accuracy advantage (last 5 turns), safe mean\n    def _avg_acc_lastN(timeline, who, n=5):\n        seq = timeline[-n:] if len(timeline) >= n else timeline\n        accs = []\n        for t in seq:\n            md = (t.get(f\"{who}_move_details\") or {})\n            a = md.get(\"accuracy\", None)\n            if isinstance(a, (int, float)):\n                accs.append(float(a))\n        return float(np.mean(accs)) if accs else 0.0\n    \n    f[\"late_acc_adv_5\"] = _avg_acc_lastN(tl, \"p1\", 5) - _avg_acc_lastN(tl, \"p2\", 5)\n\n    # --- Move-based features (full & 5 turns) ---\n    mv1_full = _move_stats_for_side(tl, \"p1\", None)\n    mv2_full = _move_stats_for_side(tl, \"p2\", None)\n    f.update(mv1_full); f.update(mv2_full)\n    f[\"mv_power_mean_ratio\"]  = _safe_ratio(mv1_full[\"mv_p1_power_mean\"], mv2_full[\"mv_p2_power_mean\"])\n    mv1_5 = _move_stats_for_side(tl, \"p1\", 5)\n    mv2_5 = _move_stats_for_side(tl, \"p2\", 5)\n    f.update(mv1_5); f.update(mv2_5)\n    \n    f[\"mv_power_mean_ratio_5\"] = _safe_ratio(mv1_5[\"mv_p1_power_mean_5\"], mv2_5[\"mv_p2_power_mean_5\"])\n    \n    # Matchup / switches / hazards / momentum / recovery / STAB / early / priority\n    f.update(_p1_vs_p2lead_matchup_index(r, tl))\n    f[\"switch_p1_count\"]=_switch_count(tl,\"p1\"); f[\"switch_p2_count\"]=_switch_count(tl,\"p2\")\n    f[\"switch_count_diff\"]=f[\"switch_p1_count\"]-f[\"switch_p2_count\"]\n    f.update(_hazard_flags(tl))\n    f.update(_recovery_pressure(tl))\n    f.update(_stab_features(r, max_turns=30))\n    f.update(_early_momentum_features(r, first_n=3))\n    f.update(_priority_feature_block(r))\n    f.update(new_features(r))\n    f.update(new_features_deb(r))\n    f.update(new_features_mirko(r))\n\n    # Team Winrate / HP resilience / Avg damage\n    try:\n        p1_team_names=_pnames_from_p1_team(r)\n        f[\"p1_team_winrate_score\"]=team_score_from_stats(p1_team_names, POKEMON_STATS, default_wr=0.5)\n    except Exception:\n        f[\"p1_team_winrate_score\"]=0.5\n    p1_names=_pnames_from_p1_team(r)\n    p2_name=_pname_from_p2_lead(r)\n    f[\"p1_team_avg_hp_score\"]=team_hp_score(p1_names, POKEMON_HP_STATS)\n    f[\"p2_lead_avg_hp\"]=POKEMON_HP_STATS.get(p2_name,{}).get(\"hp_mean\",50.0)\n    f[\"hp_resilience_diff\"]=f[\"p1_team_avg_hp_score\"] - f[\"p2_lead_avg_hp\"]\n    f[\"predicted_win_by_hp\"]=1.0 if f[\"hp_resilience_diff\"]>0 else 0.0\n\n    f.update(damage_feature_for_battle(r, pokemon_avg_damage))\n\n    # IDs / target\n    f[\"battle_id\"]=r.get(\"battle_id\")\n    if \"player_won\" in r:\n        f[\"player_won\"]= int(r[\"player_won\"]) if isinstance(r[\"player_won\"], bool) else r[\"player_won\"]\n    return f\n\n# ---------------------------------------------\n# Public API (same name & return type as starter)\n# ---------------------------------------------\ndef create_simple_features(data: list[dict]) -> pd.DataFrame:\n    rows=[]\n    for battle in tqdm(data, desc=\"Extracting features\"):\n        rows.append(_one_record_features(battle))\n    return pd.DataFrame(rows).fillna(0)\n\n# ---------------------------------------------\n# Run feature extraction\n# ---------------------------------------------\nprint(\"Processing training data...\")\ntrain_df = create_simple_features(train_data)\n\nprint(\"\\nProcessing test data...\")\ntest_data = []\nwith open(test_file_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        test_data.append(json.loads(line))\ntest_df = create_simple_features(test_data)\n\nprint(\"\\nTraining features preview:\")\n\n# --- Manual interactions (robust to missing columns) ---\ndef _maybe_add_interactions(df: pd.DataFrame) -> pd.DataFrame:\n    def safe_mul(a, b, name):\n        if a in df.columns and b in df.columns:\n            df[name] = df[a] * df[b]\n\n    # Team strength Ã— move power (full)\n    safe_mul(\"p1_team_stat_avg\", \"mv_p1_power_mean\", \"ix_p1avg_x_p1pow\")\n    # Speed Ã— priority advantage (first 5 turns if available)\n    if \"spe_max_adv\" in df.columns and \"mv_priority_count_diff_5\" in df.columns:\n        df[\"ix_speed_x_prio5\"] = df[\"spe_max_adv\"] * df[\"mv_priority_count_diff_5\"]\n    # HP momentum Ã— fraction of advantaged turns\n    safe_mul(\"tl_hp_diff_mean\", \"tl_frac_turns_advantage\", \"ix_hpmean_x_fracadv\")\n    # Early momentum Ã— priority diff (first 5)\n    if \"early_hp_diff_mean_3\" in df.columns and \"mv_priority_count_diff_5\" in df.columns:\n        df[\"ix_early3_x_prio5\"] = df[\"early_hp_diff_mean_3\"] * df[\"mv_priority_count_diff_5\"]\n    # STAB advantage Ã— early KO score\n    if \"stab_stab_ratio_diff_full\" in df.columns and \"early_first_ko_score_3\" in df.columns:\n        df[\"ix_stabdiff_x_firstko\"] = df[\"stab_stab_ratio_diff_full\"] * df[\"early_first_ko_score_3\"]\n    # Type effectiveness Ã— STAB (full)\n    if \"ter_p1_vs_p2lead_full\" in df.columns and \"stab_stab_ratio_diff_full\" in df.columns:\n        df[\"ix_ter_x_stab_full\"] = df[\"ter_p1_vs_p2lead_full\"] * df[\"stab_stab_ratio_diff_full\"]\n    # Type effectiveness Ã— early momentum (first 3)\n    if \"ter_p1_vs_p2lead_5\" in df.columns and \"early_hp_diff_mean_3\" in df.columns:\n        df[\"ix_ter5_x_early3\"] = df[\"ter_p1_vs_p2lead_5\"] * df[\"early_hp_diff_mean_3\"]\n    # Lead matchup Ã— early momentum\n    if \"lead_matchup_p1_index_5\" in df.columns and \"early_hp_diff_mean_3\" in df.columns:\n        df[\"ix_leadmatch5_x_early3\"] = df[\"lead_matchup_p1_index_5\"] * df[\"early_hp_diff_mean_3\"]\n    # Hazards advantage Ã— priority pressure\n    if \"hazard_flag_diff\" in df.columns and \"mv_priority_count_diff_5\" in df.columns:\n        df[\"ix_hazards_x_prio5\"] = df[\"hazard_flag_diff\"] * df[\"mv_priority_count_diff_5\"]\n    return df\n\ntrain_df = _maybe_add_interactions(train_df)\ntest_df  = _maybe_add_interactions(test_df)\n\n# Keep raw copies\ntrain_df_raw = train_df.copy()\ntest_df_raw  = test_df.copy()\n\n# ---- No scaling here (we'll scale inside the Pipeline in Cell 3.2) ----\n# 1) Make sure features are numeric, to avoid casting issues downstream\nnum_cols = [c for c in train_df.columns if c not in (\"battle_id\", \"player_won\")]\ntrain_df[num_cols] = train_df[num_cols].apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\ntest_df[num_cols]  = test_df[num_cols].apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\n\n# 2) Replace inf/-inf with NaN (safer for imputers)\ntr_vals = train_df[num_cols].to_numpy()\nte_vals = test_df[num_cols].to_numpy()\ntr_vals[~np.isfinite(tr_vals)] = np.nan\nte_vals[~np.isfinite(te_vals)] = np.nan\ntrain_df[num_cols] = tr_vals\ntest_df[num_cols]  = te_vals\n\n# 3) Clip percent-like fields to sensible bounds (do it raw, not scaled)\nnum_only = train_df.drop(columns=[\"battle_id\",\"player_won\"], errors=\"ignore\").select_dtypes(include=[np.number])\npercent_like = [x for x in num_only.columns if (\"hp\" in x.lower()) or (\"auc\" in x.lower())]\nfor c in percent_like:\n    if c in train_df.columns:\n        train_df[c] = train_df[c].clip(lower=0, upper=100)\n        test_df[c]  = test_df[c].clip(lower=0, upper=100)\n\n# 4) Flag near-constants (â‰¥99% same value) â€” info only\nnear_const = [c for c in num_only.columns if (num_only[c].nunique(dropna=True) / max(1, len(num_only)) < 0.01)]\nprint(f\"[Sanity] Near-constant features (not dropping yet): {len(near_const)}\")\n\n# === 2.x Custom predictive features (safe: no NaN, no div-by-zero) ===\n\nimport numpy as np\nimport pandas as pd\n\nEPS = 1e-6\nREPLACE_EXISTING = True  # set to False to skip creation if a feature name already exists\n\ndef _pick_first(df: pd.DataFrame, candidates, default_value=0.0):\n    \"\"\"Return the first existing column from candidates; else a float32 Series filled with default_value.\"\"\"\n    for c in candidates:\n        if c in df.columns:\n            return df[c].astype(\"float32\")\n    return pd.Series(default_value, index=df.index, dtype=\"float32\")\n\ndef _safe_div(a: pd.Series, b: pd.Series):\n    \"\"\"Elementwise safe division a/(b+EPS) with finite output.\"\"\"\n    out = a.astype(\"float32\") / (b.astype(\"float32\") + EPS)\n    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0).astype(\"float32\")\n    return out\n\ndef _ensure_float32(s: pd.Series):\n    return s.astype(\"float32\").replace([np.inf, -np.inf], 0.0).fillna(0.0)\n\ndef _normalize_acc(s: pd.Series):\n    \"\"\"If accuracy looks like [0..100], scale to [0..1].\"\"\"\n    s = _ensure_float32(s)\n    if len(s):\n        maxv = float(np.nanmax(s.values))\n    else:\n        maxv = 0.0\n    if maxv > 1.5:  # heuristically assume it's a percentage\n        s = s / 100.0\n    return s.clip(0.0, 1.0)\n\ndef _add_feature_pair(train_df, test_df, name, train_series, test_series):\n    \"\"\"Attach float32 features to both train and test with final sanitation.\"\"\"\n    if (not REPLACE_EXISTING) and (name in train_df.columns or name in test_df.columns):\n        return\n    train_df[name] = _ensure_float32(train_series)\n    test_df[name]  = _ensure_float32(test_series)\n\n# --- Robust base columns (try multiple candidates, fall back to zeros) ---\n\n# Attack / Defense (means)\natk_p1 = _pick_first(train_df, [\"atk_p1_mean\",\"atk_p1\",\"atk_p1_full\"], 0.0)\natk_p2 = _pick_first(train_df, [\"atk_p2_mean\",\"atk_p2\",\"atk_p2_full\"], 0.0)\ndef_p1 = _pick_first(train_df, [\"def_p1_mean\",\"def_p1\",\"def_p1_full\"], 0.0)\ndef_p2 = _pick_first(train_df, [\"def_p2_mean\",\"def_p2\",\"def_p2_full\"], 0.0)\n\natk_p1_te = _pick_first(test_df, [\"atk_p1_mean\",\"atk_p1\",\"atk_p1_full\"], 0.0)\natk_p2_te = _pick_first(test_df, [\"atk_p2_mean\",\"atk_p2\",\"atk_p2_full\"], 0.0)\ndef_p1_te = _pick_first(test_df, [\"def_p1_mean\",\"def_p1\",\"def_p1_full\"], 0.0)\ndef_p2_te = _pick_first(test_df, [\"def_p2_mean\",\"def_p2\",\"def_p2_full\"], 0.0)\n\n# Special Attack / Defense (means)\nsp_atk_p1 = _pick_first(train_df, [\"sp_atk_p1_mean\",\"spatk_p1_mean\",\"spa_p1_mean\",\"sp_atk_p1\"], 0.0)\nsp_atk_p2 = _pick_first(train_df, [\"sp_atk_p2_mean\",\"spatk_p2_mean\",\"spa_p2_mean\",\"sp_atk_p2\"], 0.0)\nsp_def_p1 = _pick_first(train_df, [\"sp_def_p1_mean\",\"spdef_p1_mean\",\"spd_p1_mean_def\",\"sp_def_p1\"], 0.0)\nsp_def_p2 = _pick_first(train_df, [\"sp_def_p2_mean\",\"spdef_p2_mean\",\"spd_p2_mean_def\",\"sp_def_p2\"], 0.0)\n\nsp_atk_p1_te = _pick_first(test_df, [\"sp_atk_p1_mean\",\"spatk_p1_mean\",\"spa_p1_mean\",\"sp_atk_p1\"], 0.0)\nsp_atk_p2_te = _pick_first(test_df, [\"sp_atk_p2_mean\",\"spatk_p2_mean\",\"spa_p2_mean\",\"sp_atk_p2\"], 0.0)\nsp_def_p1_te = _pick_first(test_df, [\"sp_def_p1_mean\",\"spdef_p1_mean\",\"spd_p1_mean_def\",\"sp_def_p1\"], 0.0)\nsp_def_p2_te = _pick_first(test_df, [\"sp_def_p2_mean\",\"spdef_p2_mean\",\"spd_p2_mean_def\",\"sp_def_p2\"], 0.0)\n\n# Speed (means)\nspd_p1 = _pick_first(train_df, [\"spd_p1_mean\",\"speed_p1_mean\",\"spd_p1\"], 0.0)\nspd_p2 = _pick_first(train_df, [\"spd_p2_mean\",\"speed_p2_mean\",\"spd_p2\"], 0.0)\nspd_p1_te = _pick_first(test_df,  [\"spd_p1_mean\",\"speed_p1_mean\",\"spd_p1\"], 0.0)\nspd_p2_te = _pick_first(test_df,  [\"spd_p2_mean\",\"speed_p2_mean\",\"spd_p2\"], 0.0)\n\n# HP current / max\nhp1_cur = _pick_first(train_df, [\"hp_p1_remain\",\"hp_p1_curr\",\"hp_p1\"], 0.0)\nhp2_cur = _pick_first(train_df, [\"hp_p2_remain\",\"hp_p2_curr\",\"hp_p2\"], 0.0)\nhp1_max = _pick_first(train_df, [\"hp_p1_max\",\"hp_p1_base\",\"hp_p1_total\"], 1.0)\nhp2_max = _pick_first(train_df, [\"hp_p2_max\",\"hp_p2_base\",\"hp_p2_total\"], 1.0)\n\nhp1_cur_te = _pick_first(test_df, [\"hp_p1_remain\",\"hp_p1_curr\",\"hp_p1\"], 0.0)\nhp2_cur_te = _pick_first(test_df, [\"hp_p2_remain\",\"hp_p2_curr\",\"hp_p2\"], 0.0)\nhp1_max_te = _pick_first(test_df, [\"hp_p1_max\",\"hp_p1_base\",\"hp_p1_total\"], 1.0)\nhp2_max_te = _pick_first(test_df, [\"hp_p2_max\",\"hp_p2_base\",\"hp_p2_total\"], 1.0)\n\n# Move power / accuracy\npwr_p1 = _pick_first(train_df, [\"mv_p1_power_mean_full\",\"mv_p1_power_mean\",\"mv_power_p1_mean\"], 0.0)\npwr_p2 = _pick_first(train_df, [\"mv_p2_power_mean_full\",\"mv_p2_power_mean\",\"mv_power_p2_mean\"], 0.0)\nacc_p1 = _normalize_acc(_pick_first(train_df, [\"mv_p1_acc_mean_full\",\"mv_p1_acc_mean\",\"mv_acc_p1_mean\"], 0.0))\nacc_p2 = _normalize_acc(_pick_first(train_df, [\"mv_p2_acc_mean_full\",\"mv_p2_acc_mean\",\"mv_acc_p2_mean\"], 0.0))\n\npwr_p1_te = _pick_first(test_df, [\"mv_p1_power_mean_full\",\"mv_p1_power_mean\",\"mv_power_p1_mean\"], 0.0)\npwr_p2_te = _pick_first(test_df, [\"mv_p2_power_mean_full\",\"mv_p2_power_mean\",\"mv_power_p2_mean\"], 0.0)\nacc_p1_te = _normalize_acc(_pick_first(test_df, [\"mv_p1_acc_mean_full\",\"mv_p1_acc_mean\",\"mv_acc_p1_mean\"], 0.0))\nacc_p2_te = _normalize_acc(_pick_first(test_df, [\"mv_p2_acc_mean_full\",\"mv_p2_acc_mean\",\"mv_acc_p2_mean\"], 0.0))\n\n# Move type counts (STATUS / PHYSICAL / SPECIAL) â€” safe fallbacks\nst_p1 = _pick_first(train_df, [\"mv_p1_count_STATUS_full\",\"mv_p1_count_STATUS\",\"status_moves_p1\"], 0.0)\nph_p1 = _pick_first(train_df, [\"mv_p1_count_PHYSICAL_full\",\"mv_p1_count_PHYSICAL\",\"physical_moves_p1\"], 0.0)\nsp_p1 = _pick_first(train_df, [\"mv_p1_count_SPECIAL_full\",\"mv_p1_count_SPECIAL\",\"special_moves_p1\"], 0.0)\nst_p2 = _pick_first(train_df, [\"mv_p2_count_STATUS_full\",\"mv_p2_count_STATUS\",\"status_moves_p2\"], 0.0)\nph_p2 = _pick_first(train_df, [\"mv_p2_count_PHYSICAL_full\",\"mv_p2_count_PHYSICAL\",\"physical_moves_p2\"], 0.0)\nsp_p2 = _pick_first(train_df, [\"mv_p2_count_SPECIAL_full\",\"mv_p2_count_SPECIAL\",\"special_moves_p2\"], 0.0)\n\nst_p1_te = _pick_first(test_df, [\"mv_p1_count_STATUS_full\",\"mv_p1_count_STATUS\",\"status_moves_p1\"], 0.0)\nph_p1_te = _pick_first(test_df, [\"mv_p1_count_PHYSICAL_full\",\"mv_p1_count_PHYSICAL\",\"physical_moves_p1\"], 0.0)\nsp_p1_te = _pick_first(test_df, [\"mv_p1_count_SPECIAL_full\",\"mv_p1_count_SPECIAL\",\"special_moves_p1\"], 0.0)\nst_p2_te = _pick_first(test_df, [\"mv_p2_count_STATUS_full\",\"mv_p2_count_STATUS\",\"status_moves_p2\"], 0.0)\nph_p2_te = _pick_first(test_df, [\"mv_p2_count_PHYSICAL_full\",\"mv_p2_count_PHYSICAL\",\"physical_moves_p2\"], 0.0)\nsp_p2_te = _pick_first(test_df, [\"mv_p2_count_SPECIAL_full\",\"mv_p2_count_SPECIAL\",\"special_moves_p2\"], 0.0)\n\n# ===============================\n# 10 SAFE, HIGH-SIGNAL FEATURES\n# ===============================\n\n# 1) atk_def_ratio: P1 attack vs P2 defense\n_add_feature_pair(\n    train_df, test_df, \"atk_def_ratio\",\n    _safe_div(atk_p1, def_p2),\n    _safe_div(atk_p1_te, def_p2_te)\n)\n\n# 2) spd_gap: P1 speed minus P2 speed\n_add_feature_pair(\n    train_df, test_df, \"spd_gap\",\n    (spd_p1 - spd_p2),\n    (spd_p1_te - spd_p2_te)\n)\n\n# 3) hp_ratio: P1 current HP vs P2 current HP\n_add_feature_pair(\n    train_df, test_df, \"hp_ratio\",\n    _safe_div(hp1_cur, hp2_cur),\n    _safe_div(hp1_cur_te, hp2_cur_te)\n)\n\n# 4) survival_score: (P1 HP%) - (P2 HP%)\n_add_feature_pair(\n    train_df, test_df, \"survival_score\",\n    _safe_div(hp1_cur, hp1_max) - _safe_div(hp2_cur, hp2_max),\n    _safe_div(hp1_cur_te, hp1_max_te) - _safe_div(hp2_cur_te, hp2_max_te)\n)\n\n# 5) momentum_index: (atk*spd)_P1 / (atk*spd)_P2\n_add_feature_pair(\n    train_df, test_df, \"momentum_index\",\n    _safe_div(atk_p1 * spd_p1, atk_p2 * spd_p2),\n    _safe_div(atk_p1_te * spd_p1_te, atk_p2_te * spd_p2_te)\n)\n\n# 6) power_acc_gap: (avg power weighted by acc) P1 - P2\npwa_p1 = _ensure_float32(pwr_p1 * acc_p1)\npwa_p2 = _ensure_float32(pwr_p2 * acc_p2)\npwa_p1_te = _ensure_float32(pwr_p1_te * acc_p1_te)\npwa_p2_te = _ensure_float32(pwr_p2_te * acc_p2_te)\n_add_feature_pair(\n    train_df, test_df, \"power_acc_gap\",\n    (pwa_p1 - pwa_p2),\n    (pwa_p1_te - pwa_p2_te)\n)\n\n# 7) offensive_balance: (atk + sp_atk) P1 / P2\n_add_feature_pair(\n    train_df, test_df, \"offensive_balance\",\n    _safe_div(atk_p1 + sp_atk_p1, atk_p2 + sp_atk_p2),\n    _safe_div(atk_p1_te + sp_atk_p1_te, atk_p2_te + sp_atk_p2_te)\n)\n\n# 8) defensive_efficiency: (def + sp_def) P1 / P2\n_add_feature_pair(\n    train_df, test_df, \"defensive_efficiency\",\n    _safe_div(def_p1 + sp_def_p1, def_p2 + sp_def_p2),\n    _safe_div(def_p1_te + sp_def_p1_te, def_p2_te + sp_def_p2_te)\n)\n\n# 9) status_influence: share STATUS moves P1 - P2\ntot_p1 = _ensure_float32(st_p1 + ph_p1 + sp_p1).replace(0.0, 1.0)\ntot_p2 = _ensure_float32(st_p2 + ph_p2 + sp_p2).replace(0.0, 1.0)\ntot_p1_te = _ensure_float32(st_p1_te + ph_p1_te + sp_p1_te).replace(0.0, 1.0)\ntot_p2_te = _ensure_float32(st_p2_te + ph_p2_te + sp_p2_te).replace(0.0, 1.0)\n\nstatus_share_p1 = _safe_div(st_p1, tot_p1)\nstatus_share_p2 = _safe_div(st_p2, tot_p2)\nstatus_share_p1_te = _safe_div(st_p1_te, tot_p1_te)\nstatus_share_p2_te = _safe_div(st_p2_te, tot_p2_te)\n\n_add_feature_pair(\n    train_df, test_df, \"status_influence\",\n    (status_share_p1 - status_share_p2),\n    (status_share_p1_te - status_share_p2_te)\n)\n\n# 10) speed_ratio: P1 speed / P2 speed\n_add_feature_pair(\n    train_df, test_df, \"speed_ratio\",\n    _safe_div(spd_p1, spd_p2),\n    _safe_div(spd_p1_te, spd_p2_te)\n)\n\n# --- Quick validation: no NaN/Inf and report how many were added ---\nnew_cols = [\n    \"atk_def_ratio\",\"spd_gap\",\"hp_ratio\",\"survival_score\",\"momentum_index\",\n    \"power_acc_gap\",\"offensive_balance\",\"defensive_efficiency\",\"status_influence\",\"speed_ratio\"\n]\nbad_train = train_df[new_cols].isna().sum().sum() + np.isinf(train_df[new_cols].to_numpy()).sum()\nbad_test  = test_df[new_cols].isna().sum().sum()  + np.isinf(test_df[new_cols].to_numpy()).sum()\nprint(f\"[FeatureEng] Added {len(new_cols)} engineered features. Bad values -> train: {bad_train}, test: {bad_test}\")\n\nprint(\"\\nPreview (raw):\")\ndisplay(train_df_raw.head())\n\nprint(\"\\nPrepared (unscaled, clean types):\")\ndisplay(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T22:02:36.427872Z","iopub.execute_input":"2025-11-12T22:02:36.428199Z","iopub.status.idle":"2025-11-12T22:03:30.811572Z","shell.execute_reply.started":"2025-11-12T22:02:36.428170Z","shell.execute_reply":"2025-11-12T22:03:30.810702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.A Overfitting check","metadata":{"_uuid":"cd2138cc-e20c-4963-91a9-2949b205cb22","_cell_guid":"180522a2-d6d8-433d-9327-d29107854e97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# === Cell A: overfitting diagnostics (learning curve) ===\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold, learning_curve, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\ndef diagnose_overfitting(\n    X, y,\n    cv_splits: int = 5,\n    train_sizes = np.linspace(0.1, 1.0, 6),\n    random_state: int = 42,\n    max_iter: int = 1000,\n    plot: bool = True\n):\n    \"\"\"\n    Computes a learning curve for a Logistic Regression pipeline (Scaler + Logistic).\n    Returns a dict with summary stats and optionally plots train vs validation accuracy.\n    \"\"\"\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"clf\", LogisticRegression(max_iter=max_iter, random_state=random_state))\n    ])\n\n    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n\n    # learning_curve supports shuffle/random_state in recent sklearn versions\n    ts_abs, train_scores, val_scores = learning_curve(\n        estimator=pipe,\n        X=X, y=y,\n        train_sizes=train_sizes,\n        cv=cv,\n        scoring=\"accuracy\",\n        shuffle=True,\n        random_state=random_state,\n        n_jobs=-1\n    )\n\n    train_mean = train_scores.mean(axis=1)\n    train_std  = train_scores.std(axis=1)\n    val_mean   = val_scores.mean(axis=1)\n    val_std    = val_scores.std(axis=1)\n\n    # Table view\n    df_lc = pd.DataFrame({\n        \"train_size\": ts_abs,\n        \"train_acc_mean\": train_mean,\n        \"train_acc_std\": train_std,\n        \"val_acc_mean\": val_mean,\n        \"val_acc_std\": val_std,\n        \"gap_train_minus_val\": train_mean - val_mean\n    })\n    display(df_lc)\n\n    # Simple decision rule for potential overfitting (gap at largest size)\n    gap_last = float(df_lc[\"gap_train_minus_val\"].iloc[-1])\n    val_last = float(df_lc[\"val_acc_mean\"].iloc[-1])\n    flag_overfit = (gap_last >= 0.05) and (val_last < 0.80 or gap_last > 0.07)\n\n    print(f\"\\nLargest train size = {int(ts_abs[-1])}\")\n    print(f\"  â€¢ Train acc (mean): {train_mean[-1]:.4f}\")\n    print(f\"  â€¢ Val   acc (mean): {val_mean[-1]:.4f}\")\n    print(f\"  â€¢ Gap (train - val): {gap_last:.4f}\")\n    print(f\"\\nPotential overfitting: {'YES' if flag_overfit else 'NO'}\")\n\n    if plot:\n        plt.figure()\n        plt.plot(ts_abs, train_mean, marker=\"o\", label=\"Train acc\")\n        plt.fill_between(ts_abs, train_mean-train_std, train_mean+train_std, alpha=0.15)\n        plt.plot(ts_abs, val_mean, marker=\"o\", label=\"Validation acc\")\n        plt.fill_between(ts_abs, val_mean-val_std, val_mean+val_std, alpha=0.15)\n        plt.xlabel(\"Number of training samples\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"Learning curve â€” Logistic (scaled)\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    return {\n        \"train_sizes\": ts_abs,\n        \"train_acc_mean\": train_mean,\n        \"val_acc_mean\": val_mean,\n        \"gap_last\": gap_last,\n        \"val_last\": val_last,\n        \"overfitting_flag\": flag_overfit\n    }\n\n# --- How to call (run after Cell 2) ---\nfeatures = [c for c in train_df.columns if c not in (\"battle_id\", \"player_won\")]\nX = train_df[features].values\ny = train_df[\"player_won\"].values\n_ = diagnose_overfitting(X, y, cv_splits=5, max_iter=1000, plot=True)","metadata":{"_uuid":"4bb2e0fc-f0ff-4bf2-a613-c51fe38e3103","_cell_guid":"98c72025-2a1c-4017-99a6-671973d4a009","trusted":true,"collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T22:03:30.812512Z","iopub.execute_input":"2025-11-12T22:03:30.813108Z","iopub.status.idle":"2025-11-12T22:03:38.593260Z","shell.execute_reply.started":"2025-11-12T22:03:30.813081Z","shell.execute_reply":"2025-11-12T22:03:38.592280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Models Training","metadata":{"_uuid":"bf65b3bb-827a-47f0-8dd3-f95e03651f9b","_cell_guid":"08ffc5f5-2dcb-437a-9e3a-62dafe9e3e9e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 3.1 - Best Features Selection, V.3 (Elastic Net + SelectFromModel)","metadata":{}},{"cell_type":"code","source":"# === 3.1 BEST FEATURES SELECTION (Elastic Net + SelectFromModel) ==================\n# - Prints original feature count\n# - Train-only pruning:\n#     (A) correlation pruning (|Ï| > 0.95)\n#     (C) optional robust VIF pruning (iterative, safe)\n# - Selector: Elastic Net (LogisticRegressionCV, saga) + SelectFromModel with a small threshold sweep\n# - Outputs: train_reduced, test_reduced with the exact selected feature subset\n# ================================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# -----------------------\n# Config\n# -----------------------\nRANDOM_STATE = 42\nID_COLS      = [\"battle_id\"]\nTARGET_COL   = \"player_won\"\nDROP_VIF     = True           # set to False to skip VIF pruning\nVIF_THRESHOLD= 25.0           # robust, not too aggressive\nMAX_VIF_STEPS= 50             # safety cap\nCORR_THR     = 0.99           # correlation pruning threshold (abs Pearson)\nNEAR_CONST_P = 0.002        # near-constant if <1% distinct values\nCV_SPLITS    = 5\n\nassert TARGET_COL in train_df.columns, f\"Target '{TARGET_COL}' not found in train_df.\"\n\n# -----------------------\n# 0) Build numeric matrices\n# -----------------------\nnum_cols_all = [c for c in train_df.columns if c not in (TARGET_COL, *ID_COLS)]\nX = train_df[num_cols_all].copy()\ny = train_df[TARGET_COL].astype(int).copy()\nX_test_full = test_df[num_cols_all].copy()\n\n# ensure numeric dtype (and keep NaN)\nX = X.apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\nX_test_full = X_test_full.apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\n\norig_feat_count = X.shape[1]\nprint(f\"[Info] Original feature count (numeric, pre-pruning): {orig_feat_count}\")\n\n# -----------------------\n# Helper: simple impute/scale (fit on TRAIN only)\n# -----------------------\ndef fit_imputer_scaler(df: pd.DataFrame):\n    imp = SimpleImputer(strategy=\"median\")\n    sca = RobustScaler()\n    Z   = imp.fit_transform(df)\n    Z   = sca.fit_transform(Z)\n    return imp, sca, Z\n\n# -----------------------\n# (A) Constants \n# -----------------------\n# constants: zero variance on observed values (ignoring NaN)\nconst_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n\nprint(f\"[Pruning][A] Constant features removed: {len(const_cols)}\")\nif const_cols:\n    print(\"  -> Constant list (first 50):\", const_cols[:50])\n\ndrop_A = const_cols\nif drop_A:\n    X.drop(columns=drop_A, inplace=True, errors=\"ignore\")\n    X_test_full.drop(columns=drop_A, inplace=True, errors=\"ignore\")\n\nprint(f\"[Pruning][A] After constant pruning: {X.shape[1]} features\")\n\n# -----------------------\n# (B) Correlation pruning (|Ï| > CORR_THR)\n# -----------------------\n# compute correlation on imputed values to avoid NaNs\nimp_tmp = SimpleImputer(strategy=\"median\").fit(X)\nX_imp = pd.DataFrame(imp_tmp.transform(X), columns=X.columns, index=X.index)\n\ncorr = X_imp.corr(method=\"pearson\").abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop_corr = [col for col in upper.columns if any(upper[col] > CORR_THR)]\n\nif to_drop_corr:\n    print(f\"[Pruning][B] Dropped for high correlation (>|Ï| {CORR_THR}): {len(to_drop_corr)}\")\n    X.drop(columns=to_drop_corr, inplace=True, errors=\"ignore\")\n    X_test_full.drop(columns=to_drop_corr, inplace=True, errors=\"ignore\")\nelse:\n    print(\"[Pruning][B] No features dropped by correlation threshold.\")\n\nprint(f\"[Pruning][B] After correlation pruning: {X.shape[1]} features\")\n\n# -----------------------\n# (C) Optional robust VIF pruning (iterative)\n# -----------------------\ndef compute_vif_frame(df_std: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Compute VIF using a standard OLS-based formula on standardized, imputed data.\n    Returns a DataFrame with columns ['feature','vif'].\n    \"\"\"\n    # classic VIF formula via linear regressions:\n    # to avoid heavy statsmodels dependency & to keep it robust/fast, we use sklearn OLS fallback\n    from sklearn.linear_model import LinearRegression\n    cols = list(df_std.columns)\n    vifs = []\n    for i, col in enumerate(cols):\n        yv = df_std[col].values\n        Xv = df_std.drop(columns=[col]).values\n        # handle degenerate case: one or zero columns left\n        if Xv.shape[1] == 0:\n            vifs.append(np.inf)\n            continue\n        # fit OLS\n        lr = LinearRegression(n_jobs=None)  # OLS via least squares\n        lr.fit(Xv, yv)\n        yhat = lr.predict(Xv)\n        # R^2\n        ss_res = np.sum((yv - yhat)**2)\n        ss_tot = np.sum((yv - np.mean(yv))**2) + 1e-12\n        r2 = 1.0 - ss_res / ss_tot\n        # VIF\n        if r2 >= 0.999999:\n            vif_val = np.inf\n        else:\n            vif_val = 1.0 / max(1e-12, (1.0 - r2))\n        vifs.append(vif_val)\n    return pd.DataFrame({\"feature\": cols, \"vif\": vifs})\n\nvif_dropped = []\nif DROP_VIF and X.shape[1] > 2:\n    print(f\"[Pruning][C] Starting VIF pruning (thr={VIF_THRESHOLD}, max steps={MAX_VIF_STEPS}) ...\")\n    step = 0\n    while step < MAX_VIF_STEPS and X.shape[1] > 2:\n        # refit imputer & scaler on current columns each iteration (avoids feature-name mismatch)\n        imp_vif, sca_vif, X_std = fit_imputer_scaler(X)\n        X_std = pd.DataFrame(X_std, columns=X.columns, index=X.index)\n\n        vif_frame = compute_vif_frame(X_std)\n        max_row = vif_frame.loc[vif_frame[\"vif\"].idxmax()]\n        max_feat, max_vif = str(max_row[\"feature\"]), float(max_row[\"vif\"])\n\n        if not np.isfinite(max_vif):\n            # drop the one producing inf VIF\n            print(f\"  [VIF] Step {step+1}: dropping '{max_feat}' (VIF=inf)\")\n            vif_dropped.append(max_feat)\n            X.drop(columns=[max_feat], inplace=True, errors=\"ignore\")\n            X_test_full.drop(columns=[max_feat], inplace=True, errors=\"ignore\")\n            step += 1\n            continue\n\n        if max_vif <= VIF_THRESHOLD:\n            print(f\"  [VIF] All VIF <= {VIF_THRESHOLD:.1f} (max={max_vif:.2f}). Stopping.\")\n            break\n\n        print(f\"  [VIF] Step {step+1}: dropping '{max_feat}' (VIF={max_vif:.2f})\")\n        vif_dropped.append(max_feat)\n        X.drop(columns=[max_feat], inplace=True, errors=\"ignore\")\n        X_test_full.drop(columns=[max_feat], inplace=True, errors=\"ignore\")\n        step += 1\n\n    print(f\"[Pruning][C] VIF dropped: {len(vif_dropped)}\")\nelse:\n    print(\"[Pruning][C] VIF pruning skipped.\")\n\nprint(f\"[Pruning] After A+B(+C): {X.shape[1]} features\")\n\nfeat_cols_after_pruning = list(X.columns)\n\n# -----------------------\n# Elastic Net selector (LogisticRegressionCV, saga)\n# -----------------------\ncv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\n# Fit imputer & scaler on TRAIN (post-pruning)\nimp_sel, sca_sel, X_std_sel = fit_imputer_scaler(X)\n\n# ENet with CV on Cs and l1 ratios\nenet = LogisticRegressionCV(\n    penalty=\"elasticnet\",\n    solver=\"saga\",\n    Cs=[0.05, 0.1, 0.2, 0.5, 1.0],\n    l1_ratios=[0.1, 0.3, 0.5, 0.7],\n    scoring=\"accuracy\",\n    cv=cv,\n    max_iter=5000,\n    tol=1e-3,\n    n_jobs=-1,\n    refit=True,\n    random_state=RANDOM_STATE\n)\nenet.fit(X_std_sel, y)\n\n# --- Top-N selection by absolute EN coefficients (replaces SelectFromModel sweep) ---\nTARGET_N_FEATURES = 60  # <- set your desired count\n\n# 1) Get absolute coefficients from the fitted CV ElasticNet (binary -> shape (1, n_features))\nabs_w = np.abs(enet.coef_.ravel())\n\n# 2) If too few non-zeros, gently relax by re-fitting once with larger C or lower l1_ratio\nnonzero_idx = np.where(abs_w > 0)[0]\nif nonzero_idx.size < TARGET_N_FEATURES:\n    # pick relaxed hyperparams from the best CV solution\n    relaxed_C = float(enet.C_[0] * 2.0)\n    relaxed_l1 = max(0.2, float(enet.l1_ratio_[0]) - 0.1)\n\n    enet_relaxed = LogisticRegressionCV(\n        penalty=\"elasticnet\",\n        solver=\"saga\",\n        Cs=[relaxed_C],\n        l1_ratios=[relaxed_l1],\n        scoring=\"accuracy\",\n        cv=cv,\n        max_iter=6000,\n        tol=1e-3,\n        n_jobs=-1,\n        refit=True,\n        random_state=RANDOM_STATE\n    )\n    enet_relaxed.fit(X_std_sel, y)\n    enet = enet_relaxed\n    abs_w = np.abs(enet.coef_.ravel())\n    nonzero_idx = np.where(abs_w > 0)[0]\n\n# 3) Pick TOP-N by absolute weight (guarantees desired count when possible)\nn_take = min(TARGET_N_FEATURES, abs_w.size)\nthresh_val = np.partition(abs_w, -n_take)[-n_take]\ntop_mask = abs_w >= thresh_val\n\nselected_cols = list(np.array(feat_cols_after_pruning)[top_mask])\nprint(f\"[ElasticNet+TopN] Non-zero weights: {nonzero_idx.size} | Selected top-{n_take}: {len(selected_cols)}\")\n\n# 4) Build reduced DataFrames (RAW view for downstream cells)\ntrain_reduced = pd.concat(\n    [train_df[ID_COLS], train_df[[TARGET_COL]], train_df[selected_cols]],\n    axis=1\n)\ntest_reduced = pd.concat(\n    [test_df[ID_COLS], test_df[selected_cols]],\n    axis=1\n)\n\nprint(f\"[Output] train_reduced shape: {train_reduced.shape} | test_reduced shape: {test_reduced.shape}\")\nprint(f\"[Features] Final selected ({len(selected_cols)}): first 25 -> {selected_cols[:25]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T22:03:38.595056Z","iopub.execute_input":"2025-11-12T22:03:38.595367Z","iopub.status.idle":"2025-11-12T22:07:28.958018Z","shell.execute_reply.started":"2025-11-12T22:03:38.595344Z","shell.execute_reply":"2025-11-12T22:07:28.957218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 - Stacking (Logistic Regression + XGBoost + Random Forest -> Logistic Regression meta)","metadata":{"_uuid":"a5923783-e209-48a4-b3a5-b3b7a38c48ca","_cell_guid":"c9df0ffe-ef6c-43d3-925a-f0ccce90af01","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# === 3.2 Stacking (LogisticRegression + XGBoost + RandomForest -> LogisticRegression meta) ===\n# # - Uses true OOF stacking: base learners trained per fold, produce OOF probs\n# # - XGBoost with early stopping (per-fold)\n# # - RF and XGB calibrated with sigmoid on the validation fold (no leakage)\n# # - Meta-learner = LogisticRegression (stable, well-calibrated on probs)\n# # - Exposes: oof_meta_scores, meta_test_scores, y\n\n# import numpy as np\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score, roc_auc_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.calibration import CalibratedClassifierCV\n\n# import xgboost as xgb\n\n# RANDOM_STATE = 99\n# FOLDS = 10\n# np.random.seed(RANDOM_STATE)\n\n# # --- Safety checks & matrices ---\n# assert \"selected_cols\" in globals(), \"Run Cell 3.1 to define 'selected_cols'.\"\n# assert \"train_reduced\" in globals() and \"test_reduced\" in globals(), \"Missing reduced frames from 3.1.\"\n\n# X_sel = train_reduced[selected_cols].to_numpy()\n# X_test_sel = test_reduced[selected_cols].to_numpy()\n# y = train_reduced[\"player_won\"].astype(int).to_numpy()\n\n# n_train = X_sel.shape[0]\n# n_test  = X_test_sel.shape[0]\n\n# print(f\"[Stack LR+XGB+RFâ†’LR] Using {X_sel.shape[1]} selected features on {n_train} training rows.\")\n\n# # --- Base learners -------------------------------------------------------------------------\n# # (1) Logistic Regression (scaled). No calibration needed.\n# base_lr = make_pipeline(\n#     StandardScaler(),\n#     LogisticRegression(\n#         solver=\"liblinear\",\n#         penalty=\"l2\",\n#         C=0.5,\n#         max_iter=3000,\n#         random_state=RANDOM_STATE\n#     )\n# )\n\n# # (2) XGBoost (with early stopping). We'll calibrate per-fold on the val fold.\n# base_xgb_params = dict(\n#     n_estimators=2000,           # high cap, early stopping will cut it\n#     learning_rate=0.03,\n#     max_depth=6,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     reg_lambda=1.0,\n#     reg_alpha=0.0,\n#     objective=\"binary:logistic\",\n#     eval_metric=\"logloss\",\n#     random_state=RANDOM_STATE,\n#     n_jobs=-1,\n#     tree_method=\"hist\"\n# )\n\n# # (3) Random Forest (regularized) + per-fold calibration\n# base_rf = RandomForestClassifier(\n#     n_estimators=400,\n#     max_depth=10,\n#     min_samples_leaf=10,\n#     max_features=\"sqrt\",\n#     bootstrap=True,\n#     n_jobs=-1,\n#     random_state=RANDOM_STATE\n# )\n\n# base_names  = [\"lr\", \"xgb\", \"rf\"]\n# n_base      = len(base_names)\n\n# # --- OOF holders for base learners (level-1 features) --------------------------------------\n# oof_base = np.zeros((n_train, n_base), dtype=float)\n# test_base_folds = np.zeros((n_test, n_base, FOLDS), dtype=float)\n\n# skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n# print(\"\\n[Per-fold validation summary]\")\n# for fold, (tr_idx, va_idx) in enumerate(skf.split(X_sel, y), 1):\n#     X_tr, X_va = X_sel[tr_idx], X_sel[va_idx]\n#     y_tr, y_va = y[tr_idx], y[va_idx]\n\n#     # ---- Base 1: Logistic Regression (no calibration) ----\n#     lr_model = make_pipeline(\n#         StandardScaler(),\n#         LogisticRegression(\n#             solver=\"liblinear\",\n#             penalty=\"l2\",\n#             C=0.5,\n#             max_iter=3000,\n#             random_state=RANDOM_STATE\n#         )\n#     )\n#     lr_model.fit(X_tr, y_tr)\n#     lr_va = lr_model.predict_proba(X_va)[:, 1]\n#     lr_te = lr_model.predict_proba(X_test_sel)[:, 1]\n\n#     # ---- Base 2: XGBoost (early stopping) + sigmoid calibration on val ----\n#     xgb_model = xgb.XGBClassifier(**base_xgb_params)\n#     xgb_model.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_va, y_va)],\n#         verbose=False\n#     )\n\n#     # predictions at best iteration (handle API differences safely)\n#     try:\n#         best_it = getattr(xgb_model, \"best_iteration\", None)\n#         if best_it is not None:\n#             xgb_va_raw = xgb_model.predict_proba(X_va, iteration_range=(0, best_it + 1))[:, 1]\n#             xgb_te_raw = xgb_model.predict_proba(X_test_sel, iteration_range=(0, best_it + 1))[:, 1]\n#         else:\n#             xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n#             xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n#         used_best = best_it\n#     except Exception:\n#         xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n#         xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n#         used_best = \"N/A\"\n\n#     # calibrate on the validation fold (no leakage)\n#     xgb_cal = CalibratedClassifierCV(estimator=xgb_model, method=\"sigmoid\", cv=\"prefit\")\n#     xgb_cal.fit(X_va, y_va)\n#     xgb_va = xgb_cal.predict_proba(X_va)[:, 1]\n#     xgb_te = xgb_cal.predict_proba(X_test_sel)[:, 1]\n\n#     # ---- Base 3: Random Forest + sigmoid calibration on val ----\n#     rf_model = RandomForestClassifier(\n#         n_estimators=400,\n#         max_depth=10,\n#         min_samples_leaf=10,\n#         max_features=\"sqrt\",\n#         bootstrap=True,\n#         n_jobs=-1,\n#         random_state=RANDOM_STATE + fold  # slight variation per fold\n#     )\n#     rf_model.fit(X_tr, y_tr)\n#     rf_cal = CalibratedClassifierCV(estimator=rf_model, method=\"sigmoid\", cv=\"prefit\")\n#     rf_cal.fit(X_va, y_va)\n#     rf_va = rf_cal.predict_proba(X_va)[:, 1]\n#     rf_te = rf_cal.predict_proba(X_test_sel)[:, 1]\n\n#     # ---- Store OOF & per-fold test probs (level-1 design) ----\n#     oof_base[va_idx, 0] = lr_va\n#     oof_base[va_idx, 1] = xgb_va\n#     oof_base[va_idx, 2] = rf_va\n\n#     test_base_folds[:, 0, fold - 1] = lr_te\n#     test_base_folds[:, 1, fold - 1] = xgb_te\n#     test_base_folds[:, 2, fold - 1] = rf_te\n\n#     # ---- Fold metrics (on validation) ----\n#     # Report each base quickly (accuracy/AUC at 0.50)\n#     def _rep(name, p):\n#         acc = accuracy_score(y_va, (p >= 0.5).astype(int))\n#         try:\n#             auc = roc_auc_score(y_va, p)\n#         except Exception:\n#             auc = np.nan\n#         return acc, auc\n\n#     acc_lr, auc_lr   = _rep(\"lr\",  lr_va)\n#     acc_xgb, auc_xgb = _rep(\"xgb\", xgb_va)\n#     acc_rf, auc_rf   = _rep(\"rf\",  rf_va)\n\n#     print(f\"  [Fold {fold}] \"\n#           f\"LR  acc={acc_lr:.4f} | AUC={auc_lr:.4f}  ||  \"\n#           f\"XGB acc={acc_xgb:.4f} | AUC={auc_xgb:.4f} | best_iter={used_best}  ||  \"\n#           f\"RF  acc={acc_rf:.4f} | AUC={auc_rf:.4f}\")\n\n# # --- Aggregate test probs across folds for each base learner ---\n# test_base_mean = test_base_folds.mean(axis=2)   # shape: (n_test, 3)\n\n# # --- Meta-learner on OOF base features (with second-level OOF for honest estimate) ----------\n# meta_clf = LogisticRegression(\n#     solver=\"lbfgs\",\n#     penalty=\"l2\",\n#     C=1.0,\n#     max_iter=5000,\n#     random_state=RANDOM_STATE\n# )\n\n# # Build true OOF for meta as well\n# oof_meta_scores = np.zeros(n_train, dtype=float)\n# meta_test_folds = np.zeros((n_test, FOLDS), dtype=float)\n\n# skf_meta = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE + 1)\n# for fold, (tr_idx, va_idx) in enumerate(skf_meta.split(oof_base, y), 1):\n#     X_tr_m, X_va_m = oof_base[tr_idx], oof_base[va_idx]\n#     y_tr_m, y_va_m = y[tr_idx], y[va_idx]\n\n#     meta_clf_fold = LogisticRegression(\n#         solver=\"lbfgs\",\n#         penalty=\"l2\",\n#         C=1.0,\n#         max_iter=5000,\n#         random_state=RANDOM_STATE + fold\n#     )\n#     meta_clf_fold.fit(X_tr_m, y_tr_m)\n#     oof_meta_scores[va_idx] = meta_clf_fold.predict_proba(X_va_m)[:, 1]\n#     meta_test_folds[:, fold - 1] = meta_clf_fold.predict_proba(test_base_mean)[:, 1]\n\n# # Final meta on full OOF (optional fit, used for reporting and stability)\n# meta_clf.fit(oof_base, y)\n# meta_test_scores = meta_test_folds.mean(axis=1)\n\n# # --- Quick OOF report for the stacked meta predictor ---\n# oof_acc_default = accuracy_score(y, (oof_meta_scores >= 0.50).astype(int))\n# try:\n#     oof_auc = roc_auc_score(y, oof_meta_scores)\n# except Exception:\n#     oof_auc = np.nan\n\n# print(\"\\n[OOF][Meta LR] Accuracy @ 0.50 = {:.4f}\".format(oof_acc_default))\n# print(\"[OOF][Meta LR] ROC-AUC = {:.4f}\".format(oof_auc))\n# print(\"\\nReady for 3.3 threshold tuning (variables: oof_meta_scores, meta_test_scores, y)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T22:07:28.958928Z","iopub.execute_input":"2025-11-12T22:07:28.959165Z","iopub.status.idle":"2025-11-12T22:09:46.558860Z","shell.execute_reply.started":"2025-11-12T22:07:28.959131Z","shell.execute_reply":"2025-11-12T22:09:46.558170Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score, roc_auc_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.calibration import CalibratedClassifierCV\n\n# import xgboost as xgb\n\n# # ===============================================================\n# # MULTI-SEED ENSEMBLE WRAPPER\n# # ===============================================================\n\n# SEEDS = [11, 42, 77, 99, 123]\n# results = []\n\n# for seed in SEEDS:\n#     print(\"\\n\" + \"=\"*80)\n#     print(f\"Running stacking with RANDOM_STATE = {seed}\")\n#     print(\"=\"*80)\n\n#     # global random seed\n#     RANDOM_STATE = seed\n#     np.random.seed(RANDOM_STATE)\n#     META_RANDOM_STATE = 42\n#     np.random.seed(META_RANDOM_STATE)\n#     FOLDS = 10\n\n#     # orginal block stars from here <------\n#     assert \"selected_cols\" in globals(), \"Run Cell 3.1 to define 'selected_cols'.\"\n#     assert \"train_reduced\" in globals() and \"test_reduced\" in globals(), \"Missing reduced frames from 3.1.\"\n    \n#     X_sel = train_reduced[selected_cols].to_numpy()\n#     X_test_sel = test_reduced[selected_cols].to_numpy()\n#     y = train_reduced[\"player_won\"].astype(int).to_numpy()\n    \n#     n_train = X_sel.shape[0]\n#     n_test  = X_test_sel.shape[0]\n    \n#     print(f\"[Stack LR+XGB+RFâ†’LR] Using {X_sel.shape[1]} selected features on {n_train} training rows.\")\n    \n#     # --- Base learners -------------------------------------------------------------------------\n#     # (1) Logistic Regression (scaled). No calibration needed.\n#     base_lr = make_pipeline(\n#         StandardScaler(),\n#         LogisticRegression(\n#             solver=\"liblinear\",\n#             penalty=\"l2\",\n#             C=0.5,\n#             max_iter=3000,\n#             random_state=RANDOM_STATE\n#         )\n#     )\n    \n#     # (2) XGBoost (with early stopping). We'll calibrate per-fold on the val fold.\n#     base_xgb_params = dict(\n#         n_estimators=2000,           # high cap, early stopping will cut it\n#         learning_rate=0.03,\n#         max_depth=6,\n#         subsample=0.8,\n#         colsample_bytree=0.8,\n#         reg_lambda=1.0,\n#         reg_alpha=0.0,\n#         objective=\"binary:logistic\",\n#         eval_metric=\"logloss\",\n#         random_state=RANDOM_STATE,\n#         n_jobs=-1,\n#         tree_method=\"hist\"\n#     )\n    \n#     # (3) Random Forest (regularized) + per-fold calibration\n#     base_rf = RandomForestClassifier(\n#         n_estimators=400,\n#         max_depth=10,\n#         min_samples_leaf=10,\n#         max_features=\"sqrt\",\n#         bootstrap=True,\n#         n_jobs=-1,\n#         random_state=RANDOM_STATE\n#     )\n    \n#     base_names  = [\"lr\", \"xgb\", \"rf\"]\n#     n_base      = len(base_names)\n    \n#     # --- OOF holders for base learners (level-1 features) --------------------------------------\n#     oof_base = np.zeros((n_train, n_base), dtype=float)\n#     test_base_folds = np.zeros((n_test, n_base, FOLDS), dtype=float)\n    \n#     skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    \n#     print(\"\\n[Per-fold validation summary]\")\n#     for fold, (tr_idx, va_idx) in enumerate(skf.split(X_sel, y), 1):\n#         X_tr, X_va = X_sel[tr_idx], X_sel[va_idx]\n#         y_tr, y_va = y[tr_idx], y[va_idx]\n    \n#         # ---- Base 1: Logistic Regression (no calibration) ----\n#         lr_model = make_pipeline(\n#             StandardScaler(),\n#             LogisticRegression(\n#                 solver=\"liblinear\",\n#                 penalty=\"l2\",\n#                 C=0.5,\n#                 max_iter=3000,\n#                 random_state=RANDOM_STATE\n#             )\n#         )\n#         lr_model.fit(X_tr, y_tr)\n#         lr_va = lr_model.predict_proba(X_va)[:, 1]\n#         lr_te = lr_model.predict_proba(X_test_sel)[:, 1]\n    \n#         # ---- Base 2: XGBoost (early stopping) + sigmoid calibration on val ----\n#         xgb_model = xgb.XGBClassifier(**base_xgb_params)\n#         xgb_model.fit(\n#             X_tr, y_tr,\n#             eval_set=[(X_va, y_va)],\n#             verbose=False\n#         )\n    \n#         # predictions at best iteration (handle API differences safely)\n#         try:\n#             best_it = getattr(xgb_model, \"best_iteration\", None)\n#             if best_it is not None:\n#                 xgb_va_raw = xgb_model.predict_proba(X_va, iteration_range=(0, best_it + 1))[:, 1]\n#                 xgb_te_raw = xgb_model.predict_proba(X_test_sel, iteration_range=(0, best_it + 1))[:, 1]\n#             else:\n#                 xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n#                 xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n#             used_best = best_it\n#         except Exception:\n#             xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n#             xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n#             used_best = \"N/A\"\n    \n#         # calibrate on the validation fold (no leakage)\n#         xgb_cal = CalibratedClassifierCV(estimator=xgb_model, method=\"sigmoid\", cv=\"prefit\")\n#         xgb_cal.fit(X_va, y_va)\n#         xgb_va = xgb_cal.predict_proba(X_va)[:, 1]\n#         xgb_te = xgb_cal.predict_proba(X_test_sel)[:, 1]\n    \n#         # ---- Base 3: Random Forest + sigmoid calibration on val ----\n#         rf_model = RandomForestClassifier(\n#             n_estimators=400,\n#             max_depth=10,\n#             min_samples_leaf=10,\n#             max_features=\"sqrt\",\n#             bootstrap=True,\n#             n_jobs=-1,\n#             random_state=RANDOM_STATE + fold  # slight variation per fold\n#         )\n#         rf_model.fit(X_tr, y_tr)\n#         rf_cal = CalibratedClassifierCV(estimator=rf_model, method=\"sigmoid\", cv=\"prefit\")\n#         rf_cal.fit(X_va, y_va)\n#         rf_va = rf_cal.predict_proba(X_va)[:, 1]\n#         rf_te = rf_cal.predict_proba(X_test_sel)[:, 1]\n    \n#         # ---- Store OOF & per-fold test probs (level-1 design) ----\n#         oof_base[va_idx, 0] = lr_va\n#         oof_base[va_idx, 1] = xgb_va\n#         oof_base[va_idx, 2] = rf_va\n    \n#         test_base_folds[:, 0, fold - 1] = lr_te\n#         test_base_folds[:, 1, fold - 1] = xgb_te\n#         test_base_folds[:, 2, fold - 1] = rf_te\n    \n#         # ---- Fold metrics (on validation) ----\n#         # Report each base quickly (accuracy/AUC at 0.50)\n#         def _rep(name, p):\n#             acc = accuracy_score(y_va, (p >= 0.5).astype(int))\n#             try:\n#                 auc = roc_auc_score(y_va, p)\n#             except Exception:\n#                 auc = np.nan\n#             return acc, auc\n    \n#         acc_lr, auc_lr   = _rep(\"lr\",  lr_va)\n#         acc_xgb, auc_xgb = _rep(\"xgb\", xgb_va)\n#         acc_rf, auc_rf   = _rep(\"rf\",  rf_va)\n    \n#         print(f\"  [Fold {fold}] \"\n#               f\"LR  acc={acc_lr:.4f} | AUC={auc_lr:.4f}  ||  \"\n#               f\"XGB acc={acc_xgb:.4f} | AUC={auc_xgb:.4f} | best_iter={used_best}  ||  \"\n#               f\"RF  acc={acc_rf:.4f} | AUC={auc_rf:.4f}\")\n    \n#     # --- Aggregate test probs across folds for each base learner ---\n#     test_base_mean = test_base_folds.mean(axis=2)   # shape: (n_test, 3)\n    \n#     # --- Meta-learner on OOF base features (with second-level OOF for honest estimate) ----------\n#     meta_clf = LogisticRegression(\n#         solver=\"lbfgs\",\n#         penalty=\"l2\",\n#         C=1.0,\n#         max_iter=5000,\n#         random_state=META_RANDOM_STATE\n#     )\n    \n#     # Build true OOF for meta as well\n#     oof_meta_scores = np.zeros(n_train, dtype=float)\n#     meta_test_folds = np.zeros((n_test, FOLDS), dtype=float)\n    \n#     skf_meta = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=META_RANDOM_STATE + 1)\n#     for fold, (tr_idx, va_idx) in enumerate(skf_meta.split(oof_base, y), 1):\n#         X_tr_m, X_va_m = oof_base[tr_idx], oof_base[va_idx]\n#         y_tr_m, y_va_m = y[tr_idx], y[va_idx]\n    \n#         meta_clf_fold = LogisticRegression(\n#             solver=\"lbfgs\",\n#             penalty=\"l2\",\n#             C=1.0,\n#             max_iter=5000,\n#             random_state=META_RANDOM_STATE + fold\n#         )\n#         meta_clf_fold.fit(X_tr_m, y_tr_m)\n#         oof_meta_scores[va_idx] = meta_clf_fold.predict_proba(X_va_m)[:, 1]\n#         meta_test_folds[:, fold - 1] = meta_clf_fold.predict_proba(test_base_mean)[:, 1]\n    \n#     # Final meta on full OOF (optional fit, used for reporting and stability)\n#     meta_clf.fit(oof_base, y)\n#     meta_test_scores = meta_test_folds.mean(axis=1)\n    \n#     # --- Quick OOF report for the stacked meta predictor ---\n#     oof_acc_default = accuracy_score(y, (oof_meta_scores >= 0.50).astype(int))\n#     try:\n#         oof_auc = roc_auc_score(y, oof_meta_scores)\n#     except Exception:\n#         oof_auc = np.nan\n    \n#     print(\"\\n[OOF][Meta LR] Accuracy @ 0.50 = {:.4f}\".format(oof_acc_default))\n#     print(\"[OOF][Meta LR] ROC-AUC = {:.4f}\".format(oof_auc))\n\n#     results.append({\n#         \"seed\": seed,\n#         \"auc\": oof_auc,\n#         \"y\": y,\n#         \"oof_meta_scores\": oof_meta_scores,\n#         \"meta_test_scores\": meta_test_scores\n#     })\n\n# # ===============================================================\n# # BEST MODEL\n# # ===============================================================\n# best_result = max(results, key=lambda x: x[\"auc\"])\n# best_seed = best_result[\"seed\"]\n# best_auc = best_result[\"auc\"]\n\n# y = best_result[\"y\"]\n# oof_meta_scores = best_result[\"oof_meta_scores\"]\n# meta_test_scores = best_result[\"meta_test_scores\"]\n\n# print(\"\\n\" + \"=\"*90)\n# print(f\"Best model found with RANDOM_STATE = {best_seed}\")\n# print(f\"Best OOF AUC = {best_auc:.4f}\")\n# print(\"=\"*90)\n# print(\"\\nReady for 3.3 threshold tuning (variables: oof_meta_scores, meta_test_scores, y)\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T22:09:46.559723Z","iopub.execute_input":"2025-11-12T22:09:46.559974Z","iopub.status.idle":"2025-11-12T22:09:46.579355Z","shell.execute_reply.started":"2025-11-12T22:09:46.559952Z","shell.execute_reply":"2025-11-12T22:09:46.578453Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nimport xgboost as xgb\n\n# ===============================================================\n# MULTI-SEED ENSEMBLE WRAPPER\n# ===============================================================\n\nSEEDS = [\n    (99, 99, 99),\n    (99,99, 200),\n    (99, 99, 110),\n    (70, 80, 200),\n    (65, 50, 250),\n    (90, 75, 180),\n    (60, 85, 220),\n    (50, 55, 160),\n    (80, 70, 210),\n    (75, 65, 190),\n]\nresults = []\n\nfor idx, (seed_lr, seed_xgb, seed_rf) in enumerate(SEEDS, 1):\n    print(\"\\n\" + \"=\"*90)\n    print(f\"Running stacking iteration {idx}\")\n    print(f\"LR_seed={seed_lr}, XGB_seed={seed_xgb}, RF_seed={seed_rf}\")\n    print(\"=\"*90)\n\n    # global random seed\n    META_RANDOM_STATE = 42\n    np.random.seed(META_RANDOM_STATE)\n    FOLDS = 10\n\n    # orginal block stars from here <------\n    assert \"selected_cols\" in globals(), \"Run Cell 3.1 to define 'selected_cols'.\"\n    assert \"train_reduced\" in globals() and \"test_reduced\" in globals(), \"Missing reduced frames from 3.1.\"\n    \n    X_sel = train_reduced[selected_cols].to_numpy()\n    X_test_sel = test_reduced[selected_cols].to_numpy()\n    y = train_reduced[\"player_won\"].astype(int).to_numpy()\n    \n    n_train = X_sel.shape[0]\n    n_test  = X_test_sel.shape[0]\n    \n    print(f\"[Stack LR+XGB+RFâ†’LR] Using {X_sel.shape[1]} selected features on {n_train} training rows.\")\n    \n    # --- Base learners -------------------------------------------------------------------------\n    # (1) Logistic Regression (scaled). No calibration needed.\n    base_lr = make_pipeline(\n        StandardScaler(),\n        LogisticRegression(\n            solver=\"liblinear\",\n            penalty=\"l2\",\n            C=0.5,\n            max_iter=3000,\n            random_state=seed_lr\n        )\n    )\n    \n    # (2) XGBoost (with early stopping). We'll calibrate per-fold on the val fold.\n    base_xgb_params = dict(\n        n_estimators=2000,           # high cap, early stopping will cut it\n        learning_rate=0.03,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=1.0,\n        reg_alpha=0.0,\n        objective=\"binary:logistic\",\n        eval_metric=\"logloss\",\n        random_state=seed_xgb,\n        n_jobs=-1,\n        tree_method=\"hist\"\n    )\n    \n    # (3) Random Forest (regularized) + per-fold calibration\n    base_rf = RandomForestClassifier(\n        n_estimators=400,\n        max_depth=10,\n        min_samples_leaf=10,\n        max_features=\"sqrt\",\n        bootstrap=True,\n        n_jobs=-1,\n        random_state=seed_rf\n    )\n    \n    base_names  = [\"lr\", \"xgb\", \"rf\"]\n    n_base      = len(base_names)\n    \n    # --- OOF holders for base learners (level-1 features) --------------------------------------\n    oof_base = np.zeros((n_train, n_base), dtype=float)\n    test_base_folds = np.zeros((n_test, n_base, FOLDS), dtype=float)\n    \n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    \n    print(\"\\n[Per-fold validation summary]\")\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_sel, y), 1):\n        X_tr, X_va = X_sel[tr_idx], X_sel[va_idx]\n        y_tr, y_va = y[tr_idx], y[va_idx]\n    \n        # ---- Base 1: Logistic Regression (no calibration) ----\n        lr_model = make_pipeline(\n            StandardScaler(),\n            LogisticRegression(\n                solver=\"liblinear\",\n                penalty=\"l2\",\n                C=0.5,\n                max_iter=3000,\n                random_state=seed_lr\n            )\n        )\n        lr_model.fit(X_tr, y_tr)\n        lr_va = lr_model.predict_proba(X_va)[:, 1]\n        lr_te = lr_model.predict_proba(X_test_sel)[:, 1]\n    \n        # ---- Base 2: XGBoost (early stopping) + sigmoid calibration on val ----\n        xgb_model = xgb.XGBClassifier(**base_xgb_params)\n        xgb_model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            verbose=False\n        )\n    \n        # predictions at best iteration (handle API differences safely)\n        try:\n            best_it = getattr(xgb_model, \"best_iteration\", None)\n            if best_it is not None:\n                xgb_va_raw = xgb_model.predict_proba(X_va, iteration_range=(0, best_it + 1))[:, 1]\n                xgb_te_raw = xgb_model.predict_proba(X_test_sel, iteration_range=(0, best_it + 1))[:, 1]\n            else:\n                xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n                xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n            used_best = best_it\n        except Exception:\n            xgb_va_raw = xgb_model.predict_proba(X_va)[:, 1]\n            xgb_te_raw = xgb_model.predict_proba(X_test_sel)[:, 1]\n            used_best = \"N/A\"\n    \n        # calibrate on the validation fold (no leakage)\n        xgb_cal = CalibratedClassifierCV(estimator=xgb_model, method=\"sigmoid\", cv=\"prefit\")\n        xgb_cal.fit(X_va, y_va)\n        xgb_va = xgb_cal.predict_proba(X_va)[:, 1]\n        xgb_te = xgb_cal.predict_proba(X_test_sel)[:, 1]\n    \n        # ---- Base 3: Random Forest + sigmoid calibration on val ----\n        rf_model = RandomForestClassifier(\n            n_estimators=400,\n            max_depth=10,\n            min_samples_leaf=10,\n            max_features=\"sqrt\",\n            bootstrap=True,\n            n_jobs=-1,\n            random_state=seed_rf + fold  # slight variation per fold\n        )\n        rf_model.fit(X_tr, y_tr)\n        rf_cal = CalibratedClassifierCV(estimator=rf_model, method=\"sigmoid\", cv=\"prefit\")\n        rf_cal.fit(X_va, y_va)\n        rf_va = rf_cal.predict_proba(X_va)[:, 1]\n        rf_te = rf_cal.predict_proba(X_test_sel)[:, 1]\n    \n        # ---- Store OOF & per-fold test probs (level-1 design) ----\n        oof_base[va_idx, 0] = lr_va\n        oof_base[va_idx, 1] = xgb_va\n        oof_base[va_idx, 2] = rf_va\n    \n        test_base_folds[:, 0, fold - 1] = lr_te\n        test_base_folds[:, 1, fold - 1] = xgb_te\n        test_base_folds[:, 2, fold - 1] = rf_te\n    \n        # ---- Fold metrics (on validation) ----\n        # Report each base quickly (accuracy/AUC at 0.50)\n        def _rep(name, p):\n            acc = accuracy_score(y_va, (p >= 0.5).astype(int))\n            try:\n                auc = roc_auc_score(y_va, p)\n            except Exception:\n                auc = np.nan\n            return acc, auc\n    \n        acc_lr, auc_lr   = _rep(\"lr\",  lr_va)\n        acc_xgb, auc_xgb = _rep(\"xgb\", xgb_va)\n        acc_rf, auc_rf   = _rep(\"rf\",  rf_va)\n    \n        print(f\"  [Fold {fold}] \"\n              f\"LR  acc={acc_lr:.4f} | AUC={auc_lr:.4f}  ||  \"\n              f\"XGB acc={acc_xgb:.4f} | AUC={auc_xgb:.4f} | best_iter={used_best}  ||  \"\n              f\"RF  acc={acc_rf:.4f} | AUC={auc_rf:.4f}\")\n    \n    # --- Aggregate test probs across folds for each base learner ---\n    test_base_mean = test_base_folds.mean(axis=2)   # shape: (n_test, 3)\n    \n    # --- Meta-learner on OOF base features (with second-level OOF for honest estimate) ----------\n    meta_clf = LogisticRegression(\n        solver=\"lbfgs\",\n        penalty=\"l2\",\n        C=1.0,\n        max_iter=5000,\n        random_state=META_RANDOM_STATE\n    )\n    \n    # Build true OOF for meta as well\n    oof_meta_scores = np.zeros(n_train, dtype=float)\n    meta_test_folds = np.zeros((n_test, FOLDS), dtype=float)\n    \n    skf_meta = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=META_RANDOM_STATE + 1)\n    for fold, (tr_idx, va_idx) in enumerate(skf_meta.split(oof_base, y), 1):\n        X_tr_m, X_va_m = oof_base[tr_idx], oof_base[va_idx]\n        y_tr_m, y_va_m = y[tr_idx], y[va_idx]\n    \n        meta_clf_fold = LogisticRegression(\n            solver=\"lbfgs\",\n            penalty=\"l2\",\n            C=1.0,\n            max_iter=5000,\n            random_state=META_RANDOM_STATE + fold\n        )\n        meta_clf_fold.fit(X_tr_m, y_tr_m)\n        oof_meta_scores[va_idx] = meta_clf_fold.predict_proba(X_va_m)[:, 1]\n        meta_test_folds[:, fold - 1] = meta_clf_fold.predict_proba(test_base_mean)[:, 1]\n    \n    # Final meta on full OOF (optional fit, used for reporting and stability)\n    meta_clf.fit(oof_base, y)\n    meta_test_scores = meta_test_folds.mean(axis=1)\n    \n    # --- Quick OOF report for the stacked meta predictor ---\n    oof_acc_default = accuracy_score(y, (oof_meta_scores >= 0.50).astype(int))\n    try:\n        oof_auc = roc_auc_score(y, oof_meta_scores)\n    except Exception:\n        oof_auc = np.nan\n    \n    print(\"\\n[OOF][Meta LR] Accuracy @ 0.50 = {:.4f}\".format(oof_acc_default))\n    print(\"[OOF][Meta LR] ROC-AUC = {:.4f}\".format(oof_auc))\n\n    results.append({\n        \"seed\": f\"LR_seed={seed_lr}, XGB_seed={seed_xgb}, RF_seed={seed_rf}\",\n        \"auc\": oof_auc,\n        \"y\": y,\n        \"oof_meta_scores\": oof_meta_scores,\n        \"meta_test_scores\": meta_test_scores\n    })\n\n# ===============================================================\n# BEST MODEL\n# ===============================================================\nbest_result = max(results, key=lambda x: x[\"auc\"])\nbest_seed = best_result[\"seed\"]\nbest_auc = best_result[\"auc\"]\n\ny = best_result[\"y\"]\noof_meta_scores = best_result[\"oof_meta_scores\"]\nmeta_test_scores = best_result[\"meta_test_scores\"]\n\nprint(\"\\n\" + \"=\"*90)\nprint(f\"Best model found with {best_seed}\")\nprint(f\"Best OOF AUC = {best_auc:.4f}\")\nprint(\"=\"*90)\nprint(\"\\nReady for 3.3 threshold tuning (variables: oof_meta_scores, meta_test_scores, y)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T23:22:51.006397Z","iopub.execute_input":"2025-11-12T23:22:51.006784Z","iopub.status.idle":"2025-11-12T23:25:06.525892Z","shell.execute_reply.started":"2025-11-12T23:22:51.006757Z","shell.execute_reply":"2025-11-12T23:25:06.525203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 - Threshold tuning for the StackingClassifier (uses OOF probs)","metadata":{"_uuid":"bdd6313f-2062-42fa-b307-393d08e9a535","_cell_guid":"ff38c3fc-7871-4c84-a702-d12a57e52704","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# === 3.3 Threshold tuning for the StackingClassifier (uses in-sample probs) ===\n# Inputs expected from 3.2: y, oof_meta_scores, meta_test_scores\n# Outputs: STACK_FINAL_THRESHOLD, STACK_FINAL_OOF_ACC, stack_pred_labels_tuned\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# --- Safety checks ---\nmissing = [name for name in [\"y\", \"oof_meta_scores\", \"meta_test_scores\"] if name not in globals()]\nif missing:\n    raise RuntimeError(f\"Missing required objects from 3.2: {missing}. Please run Cell 3.2 first.\")\n\n# --- Baseline @ 0.50 ---\noof_acc_default = accuracy_score(y, (oof_meta_scores >= 0.50).astype(int))\nprint(f\"[Stacking][OOF] Accuracy @ 0.50 = {oof_acc_default:.4f}\")\n\n# --- Coarse search over thresholds ---\nths_coarse = np.linspace(0.30, 0.70, 121)  # step 0.0033...\naccs_coarse = [accuracy_score(y, (oof_meta_scores >= t).astype(int)) for t in ths_coarse]\nbest_idx_c = int(np.argmax(accs_coarse))\nbest_thr_coarse = float(ths_coarse[best_idx_c])\nprint(f\"[Stacking][Search] Coarse best: thr={best_thr_coarse:.3f} | OOF acc={accs_coarse[best_idx_c]:.4f}\")\n\n# --- Fine search around the coarse best ---\nfine_lo = max(0.0, best_thr_coarse - 0.05)\nfine_hi = min(1.0, best_thr_coarse + 0.05)\nths_fine = np.arange(fine_lo, fine_hi + 1e-12, 0.001)\n\naccs_fine = [accuracy_score(y, (oof_meta_scores >= t).astype(int)) for t in ths_fine]\nbest_idx_f = int(np.argmax(accs_fine))\nSTACK_FINAL_THRESHOLD = float(ths_fine[best_idx_f])\nSTACK_FINAL_OOF_ACC   = float(accs_fine[best_idx_f])\n\nprint(f\"[Stacking][Best] Final OOF threshold = {STACK_FINAL_THRESHOLD:.3f} | OOF Accuracy = {STACK_FINAL_OOF_ACC:.4f}\")\n\n# --- Apply tuned threshold to TEST ---\nstack_pred_labels_tuned = (meta_test_scores >= STACK_FINAL_THRESHOLD).astype(int)\nprint(\"âœ… Created 'stack_pred_labels_tuned' for submission.\")","metadata":{"_uuid":"4ec6120f-48d6-44f2-83eb-5e69efd573cb","_cell_guid":"16776832-1ec2-414c-8bc4-2d9114e3aa43","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T22:43:48.186319Z","iopub.execute_input":"2025-11-12T22:43:48.186869Z","iopub.status.idle":"2025-11-12T22:43:48.395793Z","shell.execute_reply.started":"2025-11-12T22:43:48.186802Z","shell.execute_reply":"2025-11-12T22:43:48.394883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Creating the Submission File","metadata":{"_uuid":"fc400151-e3d6-481e-87dc-6e23ec10bb0b","_cell_guid":"fe1e5039-65ab-41a2-b17e-d0b74be840f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# === 4. Submission (StackingClassifier tuned threshold ONLY) ===\nimport pandas as pd\n\n# Safety checks\nif \"stack_pred_labels_tuned\" not in globals():\n    raise RuntimeError(\"Missing 'stack_pred_labels_tuned'. Run Cell 3.3 first.\")\nif \"test_df\" not in globals() or \"battle_id\" not in test_df.columns:\n    raise RuntimeError(\"Missing 'test_df' with 'battle_id' column.\")\n\nsubmission = pd.DataFrame({\n    \"battle_id\": test_df[\"battle_id\"].values,\n    \"player_won\": stack_pred_labels_tuned.astype(int)\n})\n\nsave_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(save_path, index=False)\nnote = f\"StackingClassifier (LR+XGB+RF â†’ LR meta), tuned thr={float(STACK_FINAL_THRESHOLD):.3f}\"\n\nprint(f\"Submission saved to {save_path}\")\nprint(f\"Note: {note}\")\ndisplay(submission.head())\n","metadata":{"_uuid":"536ccc01-19ab-4d61-b21b-d403562a08dc","_cell_guid":"2a35849f-8198-4a64-9eb1-25bffb277291","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T22:44:20.639316Z","iopub.execute_input":"2025-11-12T22:44:20.639613Z","iopub.status.idle":"2025-11-12T22:44:20.680427Z","shell.execute_reply.started":"2025-11-12T22:44:20.639592Z","shell.execute_reply":"2025-11-12T22:44:20.679634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Submitting Your Results\n\nOnce you have generated your `submission.csv` file, there are two primary ways to submit it to the competition.\n\n---\n\n#### Method A: Submitting Directly from the Notebook\n\nThis is the standard method for code competitions. It ensures that your submission is linked to the code that produced it, which is crucial for reproducibility.\n\n1.  **Save Your Work:** Click the **\"Save Version\"** button in the top-right corner of the notebook editor.\n2.  **Run the Notebook:** In the pop-up window, select **\"Save & Run All (Commit)\"** and then click the **\"Save\"** button. This will run your entire notebook from top to bottom and save the output, including your `submission.csv` file.\n3.  **Go to the Viewer:** Once the save process is complete, navigate to the notebook viewer page. \n4.  **Submit to Competition:** In the viewer, find the **\"Submit to Competition\"** section. This is usually located in the header of the output section or in the vertical \"...\" menu on the right side of the page. Clicking the **Submit** button this will submit your generated `submission.csv` file.\n\nAfter submitting, you will see your score in the **\"Submit to Competition\"** section or in the [Public Leaderboard](https://www.kaggle.com/competitions/fds-pokemon-battles-prediction-2025/leaderboard?).\n\n---\n\n#### Method B: Manual Upload\n\nYou can also generate your predictions and submission file using any environment you prefer (this notebook, Google Colab, or your local machine).\n\n1.  **Generate the `submission.csv` file** using your model.\n2.  **Download the file** to your computer.\n3.  **Navigate to the [Leaderboard Page](https://www.kaggle.com/competitions/fds-pokemon-battles-prediction-2025/leaderboard?)** and click on the **\"Submit Predictions\"** button.\n4.  **Upload Your File:** Drag and drop or select your `submission.csv` file to upload it.\n\nThis method is quick, but keep in mind that for the final evaluation, you might be required to provide the code that generated your submission.\n\nGood luck!","metadata":{"_uuid":"06a20782-709c-467d-b15a-c98f0653a2af","_cell_guid":"cd0077c1-c58f-4486-86d3-b2b38ecc0f55","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}